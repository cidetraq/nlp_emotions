{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Predicting Star Ratings from Amazon Review Titles</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('amazon_reviews_utf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Star Rating</th>\n",
       "      <th>Verifed Review</th>\n",
       "      <th>Helpful Vote Count</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Author Link</th>\n",
       "      <th>Review Link</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So far, so good (and dry)</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>25</td>\n",
       "      <td>November 9, 2016</td>\n",
       "      <td>S. R. Southard</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3V8GI...</td>\n",
       "      <td>I had my first opportunity to use the umbrella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not really a lifetime warranty...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>July 10, 2018</td>\n",
       "      <td>Rico</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1ZA3B...</td>\n",
       "      <td>I bought this umbrella a few months ago and un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"THIS IS THE ONLY UMBRELLA YOU SHOULD EVER PUR...</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>January 22, 2018</td>\n",
       "      <td>On The Fly</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RG5V0S...</td>\n",
       "      <td>First of all: I have no financial interest in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Most good quality umbrellas can't stand up to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>April 25, 2018</td>\n",
       "      <td>Linann</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2EYVB...</td>\n",
       "      <td>I work in New Haven CT, and in the fall, and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decent umbrella compromised by poor-quality ri...</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>3</td>\n",
       "      <td>October 15, 2017</td>\n",
       "      <td>Edward Ripley-Duggan</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1L9TU...</td>\n",
       "      <td>The umbrella appears to be robust; I used it r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it is still like new. Great construction</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 14, 2016</td>\n",
       "      <td>MC Oddslice</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2W29X...</td>\n",
       "      <td>Update: Too small! If there is wind, you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>For a tiny person</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>May 25, 2018</td>\n",
       "      <td>AR</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3FPDT...</td>\n",
       "      <td>I was excited to try out this umbrella based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decent, a bit pricey?</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>September 2, 2017</td>\n",
       "      <td>Alex Wang</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R12Q4A...</td>\n",
       "      <td>This umbrella is pretty good. never had a umbr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Not consistent with reviews or description.</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>3</td>\n",
       "      <td>June 20, 2017</td>\n",
       "      <td>Hannah</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1UX1K...</td>\n",
       "      <td>I must have received a bad item or something b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Seems well-built and well-designed.</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>Terende</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3C9IS...</td>\n",
       "      <td>I bought two of these for my and my spouse's u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Good Quality but Nearly Useless - Umbrella for...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>July 3, 2018</td>\n",
       "      <td>Robert S. Blackie</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R9Y91F...</td>\n",
       "      <td>I cannot argue with the fact that this umbrell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>... have to say this one is by far my favorite...</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>IJ</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3TDZY...</td>\n",
       "      <td>I've owned at least 50 umbrellas (some bought ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>classy cute umbrella</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>March 25, 2018</td>\n",
       "      <td>lawAbidingCitizen</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2PYPT...</td>\n",
       "      <td>I have benkii before(less1 year) but i lost it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Well made product, excellent vendor</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>April 7, 2018</td>\n",
       "      <td>sherm624</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2VU5A...</td>\n",
       "      <td>Change from 2 to 5 stars.  After a not-so-grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This one goes to 9...</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>January 29, 2017</td>\n",
       "      <td>Mergatroid</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1C6RZ...</td>\n",
       "      <td>Nicely built. The metal shaft is black however...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>support</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>January 23, 2018</td>\n",
       "      <td>Robert J. Moser</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2IXZT...</td>\n",
       "      <td>I am a 67 year old banker, been banking for ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Excellent umbrella- would definitely recommend</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>October 18, 2017</td>\n",
       "      <td>G Tan</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R38B28...</td>\n",
       "      <td>I like bullet points, thus: + Excellent build ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Umbrella has lots of good features</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>December 2, 2017</td>\n",
       "      <td>Lori C.</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2MWM3...</td>\n",
       "      <td>3rd party seller very involved, customer servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NOT \"super light\" or lightweight. Misleading p...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>July 20, 2017</td>\n",
       "      <td>JulieT</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R32MI0...</td>\n",
       "      <td>I was looking for a lightweight umbrella to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nope nope nope</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>June 15, 2018</td>\n",
       "      <td>BillyP</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3DZFN...</td>\n",
       "      <td>I moved to San Francisco and needed an umbrell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Not what I had hoped....</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>Krysrox413</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1TAZB...</td>\n",
       "      <td>For all the money I paid for this umbrella, I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nice while it lasted</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>January 9, 2017</td>\n",
       "      <td>Ben Perkins</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RFQF9R...</td>\n",
       "      <td>It's ok, the build quality seemed nice at firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stands up to tough weather</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 10, 2017</td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RS0OF5...</td>\n",
       "      <td>I live in Chicago suburbia so I have very litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Great size.</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>June 10, 2018</td>\n",
       "      <td>Seaside Sarah</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/ROM02O...</td>\n",
       "      <td>For a compact umbrella, it's a \"big\" umbrella ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Can withstand powerful wind</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>May 6, 2018</td>\n",
       "      <td>Zavage</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3DKA9...</td>\n",
       "      <td>Being a bachelor, I never really felt the need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Great product and customer service</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>January 3, 2017</td>\n",
       "      <td>M.G.M.</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2N49P...</td>\n",
       "      <td>The umbrella feels sturdy. Haven't experienced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Has potential - top piece unreliable</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>June 19, 2017</td>\n",
       "      <td>Kevin Lee</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R111SP...</td>\n",
       "      <td>**Update: I'm updating my review from 2 to 4 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>well-made umbrella</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>February 5, 2018</td>\n",
       "      <td>Mitch</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2TZXS...</td>\n",
       "      <td>I ordered this during a rainstorm and since it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Good value!</td>\n",
       "      <td>5</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>January 25, 2017</td>\n",
       "      <td>Clifford Carroll</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RY5UFB...</td>\n",
       "      <td>This umbrella is of better apparent quality th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sturdy and Well-Constructed</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 17, 2017</td>\n",
       "      <td>Justin Paul</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3VOD7...</td>\n",
       "      <td>Feels sturdy and a well constructed umbrella. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>December 15, 2017</td>\n",
       "      <td>jensb</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1WOIR...</td>\n",
       "      <td>Great product and fast shipping, thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>I recommend it.</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 18, 2016</td>\n",
       "      <td>Keith McCormick</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R32LUL...</td>\n",
       "      <td>It's an expensive umbrella but i assure you th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 19, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2QU4V...</td>\n",
       "      <td>Great quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 18, 2017</td>\n",
       "      <td>Jenny Larkin</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R39NX4...</td>\n",
       "      <td>Great product!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>One Star</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>November 26, 2017</td>\n",
       "      <td>Jennifer Franzone</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2QKJ0...</td>\n",
       "      <td>It was very heavy! Not what I expected.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 30, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1ZJUE...</td>\n",
       "      <td>Super product.... excellent value..buy 2!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>This is a very good umbrella and functions wel...</td>\n",
       "      <td>4</td>\n",
       "      <td>YES</td>\n",
       "      <td>On</td>\n",
       "      <td>August 29, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R36FW6...</td>\n",
       "      <td>This is a very good umbrella and functions wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 19, 2017</td>\n",
       "      <td>MattSanders</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R204XQ...</td>\n",
       "      <td>Amazing, easy to use umbrella!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>April 16, 2017</td>\n",
       "      <td>Nicholas Rodriguez</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RVUQ68...</td>\n",
       "      <td>Sturdy and built to last. These umbrellas are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 22, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RQ2D6B...</td>\n",
       "      <td>Feels very sturdy and is a decent size - bigge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>May 5, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2TGWQ...</td>\n",
       "      <td>Nice quality umbrella. Arrived earlier than ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>Great product from a company that stands behin...</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 9, 2016</td>\n",
       "      <td>CH</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R31W5S...</td>\n",
       "      <td>Holds up well in the wind. Great product from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>September 30, 2016</td>\n",
       "      <td>berta</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1UFPQ...</td>\n",
       "      <td>5 stars for this umbrella. Well made with high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>March 18, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RGAJTW...</td>\n",
       "      <td>So love this umbrella. Very easy to use. I wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>honest review</td>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 11, 2016</td>\n",
       "      <td>Richard villa olea</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3EQ9X...</td>\n",
       "      <td>cool product.i recieved for a reduced discount.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 21, 2017</td>\n",
       "      <td>Zhitun Yang</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R51QEV...</td>\n",
       "      <td>Five Star.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 9, 2016</td>\n",
       "      <td>Heather</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RJ8AM2...</td>\n",
       "      <td>Fits in my bag and keeps my hair and clothes d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>May 3, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3SUPI...</td>\n",
       "      <td>A wonderful umbrella at an excellent price!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 19, 2017</td>\n",
       "      <td>D A Sleeter</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1O2BJ...</td>\n",
       "      <td>solid construction.  haven't had rain yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>October 31, 2016</td>\n",
       "      <td>ZIX</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RQO1A8...</td>\n",
       "      <td>It is a well designed product and worth your m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 11, 2017</td>\n",
       "      <td>hudsonview</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R2V8WY...</td>\n",
       "      <td>Great umbrella!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>August 25, 2016</td>\n",
       "      <td>kim formby</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3K1FY...</td>\n",
       "      <td>I received this Umbrella free and LOVE it. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>awesome</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>July 16, 2016</td>\n",
       "      <td>caro</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R1UUCR...</td>\n",
       "      <td>The Dupont Teflon feels sturdy and keeps the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>Four Stars</td>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>April 5, 2016</td>\n",
       "      <td>Patricia</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R395TG...</td>\n",
       "      <td>love a good walk in the rain, its so relaxing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>very pleased.</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>December 11, 2016</td>\n",
       "      <td>KB</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R169R9...</td>\n",
       "      <td>These umbrellas are everything they say they a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>As Advertised</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>June 23, 2017</td>\n",
       "      <td>Corrie Kristina Luebke</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RFGLZK...</td>\n",
       "      <td>Works well so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>April 8, 2017</td>\n",
       "      <td>Rene</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/RDCU3C...</td>\n",
       "      <td>Great umbrella!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>GREAT!</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 11, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R3PODB...</td>\n",
       "      <td>Great product! Would recommend to anyone!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>May 18, 2017</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R10KGM...</td>\n",
       "      <td>Easy and well made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Four Stars</td>\n",
       "      <td>4</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>February 4, 2017</td>\n",
       "      <td>Ethan</td>\n",
       "      <td>https://amazon.com//gp/profile/amzn1.account.A...</td>\n",
       "      <td>https://amazon.com//gp/customer-reviews/R11KOI...</td>\n",
       "      <td>The umbrella is easy to open and close.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Name  Star Rating  \\\n",
       "0                             So far, so good (and dry)            5   \n",
       "1                     Not really a lifetime warranty...            2   \n",
       "2     \"THIS IS THE ONLY UMBRELLA YOU SHOULD EVER PUR...            5   \n",
       "3     Most good quality umbrellas can't stand up to ...            3   \n",
       "4     Decent umbrella compromised by poor-quality ri...            3   \n",
       "5              it is still like new. Great construction            4   \n",
       "6                                     For a tiny person            2   \n",
       "7                                 Decent, a bit pricey?            4   \n",
       "8           Not consistent with reviews or description.            2   \n",
       "9                   Seems well-built and well-designed.            5   \n",
       "10    Good Quality but Nearly Useless - Umbrella for...            2   \n",
       "11    ... have to say this one is by far my favorite...            5   \n",
       "12                                 classy cute umbrella            5   \n",
       "13                  Well made product, excellent vendor            5   \n",
       "14                                This one goes to 9...            5   \n",
       "15                                              support            4   \n",
       "16       Excellent umbrella- would definitely recommend            5   \n",
       "17                   Umbrella has lots of good features            4   \n",
       "18    NOT \"super light\" or lightweight. Misleading p...            2   \n",
       "19                                       Nope nope nope            1   \n",
       "20                             Not what I had hoped....            2   \n",
       "21                                 Nice while it lasted            3   \n",
       "22                           Stands up to tough weather            5   \n",
       "23                                          Great size.            4   \n",
       "24                          Can withstand powerful wind            5   \n",
       "25                   Great product and customer service            5   \n",
       "26                 Has potential - top piece unreliable            4   \n",
       "27                                   well-made umbrella            5   \n",
       "28                                          Good value!            5   \n",
       "29                          Sturdy and Well-Constructed            4   \n",
       "...                                                 ...          ...   \n",
       "2370                                         Five Stars            5   \n",
       "2371                                    I recommend it.            5   \n",
       "2372                                         Five Stars            5   \n",
       "2373                                         Five Stars            5   \n",
       "2374                                           One Star            1   \n",
       "2375                                         Five Stars            5   \n",
       "2376  This is a very good umbrella and functions wel...            4   \n",
       "2377                                         Five Stars            5   \n",
       "2378                                         Five Stars            5   \n",
       "2379                                         Five Stars            5   \n",
       "2380                                         Five Stars            5   \n",
       "2381  Great product from a company that stands behin...            5   \n",
       "2382                                            5 stars            5   \n",
       "2383                                         Five Stars            5   \n",
       "2384                                      honest review            4   \n",
       "2385                                         Five Stars            5   \n",
       "2386                                         Five Stars            5   \n",
       "2387                                         Five Stars            5   \n",
       "2388                                         Five Stars            5   \n",
       "2389                                         Five Stars            5   \n",
       "2390                                         Five Stars            5   \n",
       "2391                                         Five Stars            5   \n",
       "2392                                            awesome            5   \n",
       "2393                                         Four Stars            4   \n",
       "2394                                      very pleased.            5   \n",
       "2395                                      As Advertised            5   \n",
       "2396                                         Five Stars            5   \n",
       "2397                                             GREAT!            5   \n",
       "2398                                         Five Stars            5   \n",
       "2399                                         Four Stars            4   \n",
       "\n",
       "     Verifed Review Helpful Vote Count                 Date  \\\n",
       "0               YES                 25     November 9, 2016   \n",
       "1               YES                 On        July 10, 2018   \n",
       "2               YES                 On     January 22, 2018   \n",
       "3               YES                  2       April 25, 2018   \n",
       "4               YES                  3     October 15, 2017   \n",
       "5               YES                 On      August 14, 2016   \n",
       "6               YES                 On         May 25, 2018   \n",
       "7               YES                  2    September 2, 2017   \n",
       "8               YES                  3        June 20, 2017   \n",
       "9               YES                  0     January 26, 2018   \n",
       "10              YES                  0         July 3, 2018   \n",
       "11              YES                  0    February 27, 2018   \n",
       "12              YES                  0       March 25, 2018   \n",
       "13              YES                  0        April 7, 2018   \n",
       "14              YES                 On     January 29, 2017   \n",
       "15              YES                 On     January 23, 2018   \n",
       "16              YES                  0     October 18, 2017   \n",
       "17              YES                 On     December 2, 2017   \n",
       "18              YES                 On        July 20, 2017   \n",
       "19              YES                 On        June 15, 2018   \n",
       "20              YES                  0          May 2, 2018   \n",
       "21              YES                  2      January 9, 2017   \n",
       "22              YES                 On      August 10, 2017   \n",
       "23              YES                  0        June 10, 2018   \n",
       "24              YES                  0          May 6, 2018   \n",
       "25              YES                  0      January 3, 2017   \n",
       "26              YES                 On        June 19, 2017   \n",
       "27              YES                  0     February 5, 2018   \n",
       "28              YES                  0     January 25, 2017   \n",
       "29              YES                 On      August 17, 2017   \n",
       "...             ...                ...                  ...   \n",
       "2370             NO                  0    December 15, 2017   \n",
       "2371             NO                  0        June 18, 2016   \n",
       "2372             NO                  0      August 19, 2017   \n",
       "2373             NO                  0        June 18, 2017   \n",
       "2374             NO                  0    November 26, 2017   \n",
       "2375             NO                  0      August 30, 2017   \n",
       "2376            YES                 On      August 29, 2017   \n",
       "2377             NO                  0        July 19, 2017   \n",
       "2378             NO                  0       April 16, 2017   \n",
       "2379             NO                  0    February 22, 2017   \n",
       "2380             NO                  0          May 5, 2017   \n",
       "2381             NO                  0         July 9, 2016   \n",
       "2382             NO                  0   September 30, 2016   \n",
       "2383             NO                  0       March 18, 2017   \n",
       "2384             NO                  0        July 11, 2016   \n",
       "2385             NO                  0      August 21, 2017   \n",
       "2386             NO                  0         June 9, 2016   \n",
       "2387             NO                  0          May 3, 2017   \n",
       "2388             NO                  0    February 19, 2017   \n",
       "2389             NO                  0     October 31, 2016   \n",
       "2390             NO                  0        June 11, 2017   \n",
       "2391             NO                  0      August 25, 2016   \n",
       "2392             NO                  0        July 16, 2016   \n",
       "2393             NO                  0        April 5, 2016   \n",
       "2394             NO                  0    December 11, 2016   \n",
       "2395             NO                  0        June 23, 2017   \n",
       "2396             NO                  0        April 8, 2017   \n",
       "2397             NO                  0    February 11, 2017   \n",
       "2398             NO                  0         May 18, 2017   \n",
       "2399             NO                  0     February 4, 2017   \n",
       "\n",
       "                      Author  \\\n",
       "0             S. R. Southard   \n",
       "1                       Rico   \n",
       "2                 On The Fly   \n",
       "3                     Linann   \n",
       "4       Edward Ripley-Duggan   \n",
       "5                MC Oddslice   \n",
       "6                         AR   \n",
       "7                  Alex Wang   \n",
       "8                     Hannah   \n",
       "9                    Terende   \n",
       "10         Robert S. Blackie   \n",
       "11                        IJ   \n",
       "12         lawAbidingCitizen   \n",
       "13                  sherm624   \n",
       "14                Mergatroid   \n",
       "15           Robert J. Moser   \n",
       "16                     G Tan   \n",
       "17                   Lori C.   \n",
       "18                    JulieT   \n",
       "19                    BillyP   \n",
       "20                Krysrox413   \n",
       "21               Ben Perkins   \n",
       "22                Alexandria   \n",
       "23             Seaside Sarah   \n",
       "24                    Zavage   \n",
       "25                    M.G.M.   \n",
       "26                 Kevin Lee   \n",
       "27                     Mitch   \n",
       "28          Clifford Carroll   \n",
       "29               Justin Paul   \n",
       "...                      ...   \n",
       "2370                   jensb   \n",
       "2371         Keith McCormick   \n",
       "2372         Amazon Customer   \n",
       "2373            Jenny Larkin   \n",
       "2374       Jennifer Franzone   \n",
       "2375         Amazon Customer   \n",
       "2376         Amazon Customer   \n",
       "2377             MattSanders   \n",
       "2378      Nicholas Rodriguez   \n",
       "2379         Amazon Customer   \n",
       "2380         Amazon Customer   \n",
       "2381                      CH   \n",
       "2382                   berta   \n",
       "2383         Amazon Customer   \n",
       "2384      Richard villa olea   \n",
       "2385             Zhitun Yang   \n",
       "2386                 Heather   \n",
       "2387         Amazon Customer   \n",
       "2388             D A Sleeter   \n",
       "2389                     ZIX   \n",
       "2390              hudsonview   \n",
       "2391              kim formby   \n",
       "2392                    caro   \n",
       "2393                Patricia   \n",
       "2394                      KB   \n",
       "2395  Corrie Kristina Luebke   \n",
       "2396                    Rene   \n",
       "2397         Amazon Customer   \n",
       "2398         Amazon Customer   \n",
       "2399                   Ethan   \n",
       "\n",
       "                                            Author Link  \\\n",
       "0     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "1     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "3     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "4     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "5     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "6     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "7     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "8     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "9     https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "10    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "11    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "12    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "13    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "14    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "15    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "16    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "17    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "18    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "19    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "20    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "21    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "22    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "23    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "24    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "25    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "26    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "27    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "28    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "29    https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "...                                                 ...   \n",
       "2370  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2371  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2372  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2373  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2374  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2375  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2376  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2377  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2378  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2379  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2380  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2381  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2382  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2383  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2384  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2385  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2386  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2387  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2388  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2389  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2390  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2391  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2392  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2393  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2394  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2395  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2396  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2397  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2398  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "2399  https://amazon.com//gp/profile/amzn1.account.A...   \n",
       "\n",
       "                                            Review Link  \\\n",
       "0     https://amazon.com//gp/customer-reviews/R3V8GI...   \n",
       "1     https://amazon.com//gp/customer-reviews/R1ZA3B...   \n",
       "2     https://amazon.com//gp/customer-reviews/RG5V0S...   \n",
       "3     https://amazon.com//gp/customer-reviews/R2EYVB...   \n",
       "4     https://amazon.com//gp/customer-reviews/R1L9TU...   \n",
       "5     https://amazon.com//gp/customer-reviews/R2W29X...   \n",
       "6     https://amazon.com//gp/customer-reviews/R3FPDT...   \n",
       "7     https://amazon.com//gp/customer-reviews/R12Q4A...   \n",
       "8     https://amazon.com//gp/customer-reviews/R1UX1K...   \n",
       "9     https://amazon.com//gp/customer-reviews/R3C9IS...   \n",
       "10    https://amazon.com//gp/customer-reviews/R9Y91F...   \n",
       "11    https://amazon.com//gp/customer-reviews/R3TDZY...   \n",
       "12    https://amazon.com//gp/customer-reviews/R2PYPT...   \n",
       "13    https://amazon.com//gp/customer-reviews/R2VU5A...   \n",
       "14    https://amazon.com//gp/customer-reviews/R1C6RZ...   \n",
       "15    https://amazon.com//gp/customer-reviews/R2IXZT...   \n",
       "16    https://amazon.com//gp/customer-reviews/R38B28...   \n",
       "17    https://amazon.com//gp/customer-reviews/R2MWM3...   \n",
       "18    https://amazon.com//gp/customer-reviews/R32MI0...   \n",
       "19    https://amazon.com//gp/customer-reviews/R3DZFN...   \n",
       "20    https://amazon.com//gp/customer-reviews/R1TAZB...   \n",
       "21    https://amazon.com//gp/customer-reviews/RFQF9R...   \n",
       "22    https://amazon.com//gp/customer-reviews/RS0OF5...   \n",
       "23    https://amazon.com//gp/customer-reviews/ROM02O...   \n",
       "24    https://amazon.com//gp/customer-reviews/R3DKA9...   \n",
       "25    https://amazon.com//gp/customer-reviews/R2N49P...   \n",
       "26    https://amazon.com//gp/customer-reviews/R111SP...   \n",
       "27    https://amazon.com//gp/customer-reviews/R2TZXS...   \n",
       "28    https://amazon.com//gp/customer-reviews/RY5UFB...   \n",
       "29    https://amazon.com//gp/customer-reviews/R3VOD7...   \n",
       "...                                                 ...   \n",
       "2370  https://amazon.com//gp/customer-reviews/R1WOIR...   \n",
       "2371  https://amazon.com//gp/customer-reviews/R32LUL...   \n",
       "2372  https://amazon.com//gp/customer-reviews/R2QU4V...   \n",
       "2373  https://amazon.com//gp/customer-reviews/R39NX4...   \n",
       "2374  https://amazon.com//gp/customer-reviews/R2QKJ0...   \n",
       "2375  https://amazon.com//gp/customer-reviews/R1ZJUE...   \n",
       "2376  https://amazon.com//gp/customer-reviews/R36FW6...   \n",
       "2377  https://amazon.com//gp/customer-reviews/R204XQ...   \n",
       "2378  https://amazon.com//gp/customer-reviews/RVUQ68...   \n",
       "2379  https://amazon.com//gp/customer-reviews/RQ2D6B...   \n",
       "2380  https://amazon.com//gp/customer-reviews/R2TGWQ...   \n",
       "2381  https://amazon.com//gp/customer-reviews/R31W5S...   \n",
       "2382  https://amazon.com//gp/customer-reviews/R1UFPQ...   \n",
       "2383  https://amazon.com//gp/customer-reviews/RGAJTW...   \n",
       "2384  https://amazon.com//gp/customer-reviews/R3EQ9X...   \n",
       "2385  https://amazon.com//gp/customer-reviews/R51QEV...   \n",
       "2386  https://amazon.com//gp/customer-reviews/RJ8AM2...   \n",
       "2387  https://amazon.com//gp/customer-reviews/R3SUPI...   \n",
       "2388  https://amazon.com//gp/customer-reviews/R1O2BJ...   \n",
       "2389  https://amazon.com//gp/customer-reviews/RQO1A8...   \n",
       "2390  https://amazon.com//gp/customer-reviews/R2V8WY...   \n",
       "2391  https://amazon.com//gp/customer-reviews/R3K1FY...   \n",
       "2392  https://amazon.com//gp/customer-reviews/R1UUCR...   \n",
       "2393  https://amazon.com//gp/customer-reviews/R395TG...   \n",
       "2394  https://amazon.com//gp/customer-reviews/R169R9...   \n",
       "2395  https://amazon.com//gp/customer-reviews/RFGLZK...   \n",
       "2396  https://amazon.com//gp/customer-reviews/RDCU3C...   \n",
       "2397  https://amazon.com//gp/customer-reviews/R3PODB...   \n",
       "2398  https://amazon.com//gp/customer-reviews/R10KGM...   \n",
       "2399  https://amazon.com//gp/customer-reviews/R11KOI...   \n",
       "\n",
       "                                            Review Text  \n",
       "0     I had my first opportunity to use the umbrella...  \n",
       "1     I bought this umbrella a few months ago and un...  \n",
       "2     First of all: I have no financial interest in ...  \n",
       "3     I work in New Haven CT, and in the fall, and w...  \n",
       "4     The umbrella appears to be robust; I used it r...  \n",
       "5     Update: Too small! If there is wind, you will ...  \n",
       "6     I was excited to try out this umbrella based o...  \n",
       "7     This umbrella is pretty good. never had a umbr...  \n",
       "8     I must have received a bad item or something b...  \n",
       "9     I bought two of these for my and my spouse's u...  \n",
       "10    I cannot argue with the fact that this umbrell...  \n",
       "11    I've owned at least 50 umbrellas (some bought ...  \n",
       "12    I have benkii before(less1 year) but i lost it...  \n",
       "13    Change from 2 to 5 stars.  After a not-so-grea...  \n",
       "14    Nicely built. The metal shaft is black however...  \n",
       "15    I am a 67 year old banker, been banking for ov...  \n",
       "16    I like bullet points, thus: + Excellent build ...  \n",
       "17    3rd party seller very involved, customer servi...  \n",
       "18    I was looking for a lightweight umbrella to re...  \n",
       "19    I moved to San Francisco and needed an umbrell...  \n",
       "20    For all the money I paid for this umbrella, I'...  \n",
       "21    It's ok, the build quality seemed nice at firs...  \n",
       "22    I live in Chicago suburbia so I have very litt...  \n",
       "23    For a compact umbrella, it's a \"big\" umbrella ...  \n",
       "24    Being a bachelor, I never really felt the need...  \n",
       "25    The umbrella feels sturdy. Haven't experienced...  \n",
       "26    **Update: I'm updating my review from 2 to 4 s...  \n",
       "27    I ordered this during a rainstorm and since it...  \n",
       "28    This umbrella is of better apparent quality th...  \n",
       "29    Feels sturdy and a well constructed umbrella. ...  \n",
       "...                                                 ...  \n",
       "2370         Great product and fast shipping, thank you  \n",
       "2371  It's an expensive umbrella but i assure you th...  \n",
       "2372                                      Great quality  \n",
       "2373                                     Great product!  \n",
       "2374            It was very heavy! Not what I expected.  \n",
       "2375          Super product.... excellent value..buy 2!  \n",
       "2376  This is a very good umbrella and functions wel...  \n",
       "2377                     Amazing, easy to use umbrella!  \n",
       "2378  Sturdy and built to last. These umbrellas are ...  \n",
       "2379  Feels very sturdy and is a decent size - bigge...  \n",
       "2380  Nice quality umbrella. Arrived earlier than ex...  \n",
       "2381  Holds up well in the wind. Great product from ...  \n",
       "2382  5 stars for this umbrella. Well made with high...  \n",
       "2383  So love this umbrella. Very easy to use. I wou...  \n",
       "2384    cool product.i recieved for a reduced discount.  \n",
       "2385                                         Five Star.  \n",
       "2386  Fits in my bag and keeps my hair and clothes d...  \n",
       "2387        A wonderful umbrella at an excellent price!  \n",
       "2388          solid construction.  haven't had rain yet  \n",
       "2389  It is a well designed product and worth your m...  \n",
       "2390                                    Great umbrella!  \n",
       "2391  I received this Umbrella free and LOVE it. It ...  \n",
       "2392  The Dupont Teflon feels sturdy and keeps the r...  \n",
       "2393  love a good walk in the rain, its so relaxing ...  \n",
       "2394  These umbrellas are everything they say they a...  \n",
       "2395                                  Works well so far  \n",
       "2396                                    Great umbrella!  \n",
       "2397          Great product! Would recommend to anyone!  \n",
       "2398                                 Easy and well made  \n",
       "2399            The umbrella is easy to open and close.  \n",
       "\n",
       "[2400 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HashingVectorizer first using 'Name' Column</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of text documents\n",
    "text = dataset['Name']\n",
    "#input_text=[]\n",
    "#input_text.append(text)\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "#vector = vectorizer.transform(input_text)\n",
    "# summarize encoded vector\n",
    "X=vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 20)\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.25819889 ...,  0.51639778  0.25819889  0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ..., -0.70710678  0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One hot encoding y values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=dataset['Star Rating']\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y=encoder.transform(y)\n",
    "y=np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Trying an ANN first</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "def baseline_model():\n",
    "    classifier=Sequential()\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 20))\n",
    "\n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "    # Adding the output layer (Softmax because we want probabilities within 0 and 1 )\n",
    "    classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "    # Compiling the ANN (gradient descent with logarithmic loss)\n",
    "    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without wrapper\n",
    "classifier=Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 20))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer (Softmax because we want probabilities within 0 and 1 )\n",
    "classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN (gradient descent with logarithmic loss)\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1920/1920 [==============================] - 1s 359us/step - loss: 1.3822 - acc: 0.8052\n",
      "Epoch 2/100\n",
      "1920/1920 [==============================] - 0s 117us/step - loss: 0.7201 - acc: 0.8094\n",
      "Epoch 3/100\n",
      "1920/1920 [==============================] - 0s 115us/step - loss: 0.6576 - acc: 0.8094\n",
      "Epoch 4/100\n",
      "1920/1920 [==============================] - 0s 115us/step - loss: 0.6493 - acc: 0.8094\n",
      "Epoch 5/100\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.6452 - acc: 0.8094\n",
      "Epoch 6/100\n",
      "1920/1920 [==============================] - 0s 132us/step - loss: 0.6426 - acc: 0.8094\n",
      "Epoch 7/100\n",
      "1920/1920 [==============================] - 0s 143us/step - loss: 0.6406 - acc: 0.8094\n",
      "Epoch 8/100\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.6378 - acc: 0.8094\n",
      "Epoch 9/100\n",
      "1920/1920 [==============================] - 0s 114us/step - loss: 0.6366 - acc: 0.8094\n",
      "Epoch 10/100\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.6351 - acc: 0.8094\n",
      "Epoch 11/100\n",
      "1920/1920 [==============================] - 0s 121us/step - loss: 0.6341 - acc: 0.8094\n",
      "Epoch 12/100\n",
      "1920/1920 [==============================] - 0s 126us/step - loss: 0.6328 - acc: 0.8094\n",
      "Epoch 13/100\n",
      "1920/1920 [==============================] - 0s 112us/step - loss: 0.6313 - acc: 0.8094\n",
      "Epoch 14/100\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.6307 - acc: 0.8094\n",
      "Epoch 15/100\n",
      "1920/1920 [==============================] - 0s 138us/step - loss: 0.6300 - acc: 0.8094\n",
      "Epoch 16/100\n",
      "1920/1920 [==============================] - 0s 120us/step - loss: 0.6289 - acc: 0.8094\n",
      "Epoch 17/100\n",
      "1920/1920 [==============================] - 0s 127us/step - loss: 0.6281 - acc: 0.8094\n",
      "Epoch 18/100\n",
      "1920/1920 [==============================] - 0s 127us/step - loss: 0.6274 - acc: 0.8094\n",
      "Epoch 19/100\n",
      "1920/1920 [==============================] - 0s 118us/step - loss: 0.6270 - acc: 0.8094\n",
      "Epoch 20/100\n",
      "1920/1920 [==============================] - 0s 114us/step - loss: 0.6259 - acc: 0.8094\n",
      "Epoch 21/100\n",
      "1920/1920 [==============================] - 0s 112us/step - loss: 0.6249 - acc: 0.8094\n",
      "Epoch 22/100\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.6252 - acc: 0.8094\n",
      "Epoch 23/100\n",
      "1920/1920 [==============================] - 0s 123us/step - loss: 0.6238 - acc: 0.8094\n",
      "Epoch 24/100\n",
      "1920/1920 [==============================] - 0s 125us/step - loss: 0.6233 - acc: 0.8094\n",
      "Epoch 25/100\n",
      "1920/1920 [==============================] - 0s 114us/step - loss: 0.6230 - acc: 0.8094\n",
      "Epoch 26/100\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.6219 - acc: 0.8094\n",
      "Epoch 27/100\n",
      "1920/1920 [==============================] - 0s 115us/step - loss: 0.6215 - acc: 0.8094\n",
      "Epoch 28/100\n",
      "1920/1920 [==============================] - 0s 126us/step - loss: 0.6206 - acc: 0.8094\n",
      "Epoch 29/100\n",
      "1920/1920 [==============================] - 0s 119us/step - loss: 0.6203 - acc: 0.8094\n",
      "Epoch 30/100\n",
      "1920/1920 [==============================] - 0s 130us/step - loss: 0.6191 - acc: 0.8094\n",
      "Epoch 31/100\n",
      "1920/1920 [==============================] - 0s 114us/step - loss: 0.6190 - acc: 0.8094\n",
      "Epoch 32/100\n",
      "1920/1920 [==============================] - 0s 112us/step - loss: 0.6181 - acc: 0.8094\n",
      "Epoch 33/100\n",
      "1920/1920 [==============================] - 0s 112us/step - loss: 0.6176 - acc: 0.8094\n",
      "Epoch 34/100\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.6164 - acc: 0.8094\n",
      "Epoch 35/100\n",
      "1920/1920 [==============================] - 0s 112us/step - loss: 0.6159 - acc: 0.8094\n",
      "Epoch 36/100\n",
      "1920/1920 [==============================] - 0s 117us/step - loss: 0.6152 - acc: 0.8094\n",
      "Epoch 37/100\n",
      "1920/1920 [==============================] - 0s 129us/step - loss: 0.6143 - acc: 0.8094\n",
      "Epoch 38/100\n",
      "1920/1920 [==============================] - 0s 114us/step - loss: 0.6132 - acc: 0.8094\n",
      "Epoch 39/100\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.6120 - acc: 0.8094\n",
      "Epoch 40/100\n",
      "1920/1920 [==============================] - 0s 117us/step - loss: 0.6114 - acc: 0.8094\n",
      "Epoch 41/100\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.6103 - acc: 0.8094\n",
      "Epoch 42/100\n",
      "1920/1920 [==============================] - 0s 115us/step - loss: 0.6096 - acc: 0.8094\n",
      "Epoch 43/100\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.6084 - acc: 0.8094\n",
      "Epoch 44/100\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.6077 - acc: 0.8094\n",
      "Epoch 45/100\n",
      "1920/1920 [==============================] - 0s 115us/step - loss: 0.6063 - acc: 0.8094\n",
      "Epoch 46/100\n",
      "1920/1920 [==============================] - 0s 115us/step - loss: 0.6049 - acc: 0.8094\n",
      "Epoch 47/100\n",
      "1920/1920 [==============================] - 0s 126us/step - loss: 0.6045 - acc: 0.8094\n",
      "Epoch 48/100\n",
      "1920/1920 [==============================] - 0s 120us/step - loss: 0.6031 - acc: 0.8094\n",
      "Epoch 49/100\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.6025 - acc: 0.8094\n",
      "Epoch 50/100\n",
      "1920/1920 [==============================] - 0s 118us/step - loss: 0.6013 - acc: 0.8094\n",
      "Epoch 51/100\n",
      "1920/1920 [==============================] - 0s 124us/step - loss: 0.6004 - acc: 0.8094\n",
      "Epoch 52/100\n",
      "1920/1920 [==============================] - 0s 123us/step - loss: 0.5999 - acc: 0.8094\n",
      "Epoch 53/100\n",
      "1920/1920 [==============================] - 0s 130us/step - loss: 0.5983 - acc: 0.8094\n",
      "Epoch 54/100\n",
      "1920/1920 [==============================] - 0s 120us/step - loss: 0.5980 - acc: 0.8094\n",
      "Epoch 55/100\n",
      "1920/1920 [==============================] - 0s 122us/step - loss: 0.5964 - acc: 0.8094\n",
      "Epoch 56/100\n",
      "1920/1920 [==============================] - 0s 121us/step - loss: 0.5960 - acc: 0.8094\n",
      "Epoch 57/100\n",
      "1920/1920 [==============================] - 0s 119us/step - loss: 0.5943 - acc: 0.8094\n",
      "Epoch 58/100\n",
      "1920/1920 [==============================] - 0s 120us/step - loss: 0.5937 - acc: 0.8094\n",
      "Epoch 59/100\n",
      "1920/1920 [==============================] - 0s 128us/step - loss: 0.5935 - acc: 0.8094\n",
      "Epoch 60/100\n",
      "1920/1920 [==============================] - 0s 119us/step - loss: 0.5923 - acc: 0.8094\n",
      "Epoch 61/100\n",
      "1920/1920 [==============================] - 0s 121us/step - loss: 0.5913 - acc: 0.8094\n",
      "Epoch 62/100\n",
      "1920/1920 [==============================] - 0s 127us/step - loss: 0.5906 - acc: 0.8094\n",
      "Epoch 63/100\n",
      "1920/1920 [==============================] - 0s 117us/step - loss: 0.5893 - acc: 0.8094\n",
      "Epoch 64/100\n",
      "1920/1920 [==============================] - 0s 130us/step - loss: 0.5885 - acc: 0.8094\n",
      "Epoch 65/100\n",
      "1920/1920 [==============================] - 0s 124us/step - loss: 0.5876 - acc: 0.8094\n",
      "Epoch 66/100\n",
      "1920/1920 [==============================] - 0s 125us/step - loss: 0.5869 - acc: 0.8094\n",
      "Epoch 67/100\n",
      "1920/1920 [==============================] - 0s 122us/step - loss: 0.5851 - acc: 0.8094\n",
      "Epoch 68/100\n",
      "1920/1920 [==============================] - 0s 135us/step - loss: 0.5841 - acc: 0.8094\n",
      "Epoch 69/100\n",
      "1920/1920 [==============================] - 0s 129us/step - loss: 0.5831 - acc: 0.8094\n",
      "Epoch 70/100\n",
      "1920/1920 [==============================] - 0s 130us/step - loss: 0.5821 - acc: 0.8094\n",
      "Epoch 71/100\n",
      "1920/1920 [==============================] - 0s 127us/step - loss: 0.5806 - acc: 0.8094\n",
      "Epoch 72/100\n",
      "1920/1920 [==============================] - 0s 135us/step - loss: 0.5797 - acc: 0.8094\n",
      "Epoch 73/100\n",
      "1920/1920 [==============================] - 0s 128us/step - loss: 0.5783 - acc: 0.8094\n",
      "Epoch 74/100\n",
      "1920/1920 [==============================] - 0s 132us/step - loss: 0.5772 - acc: 0.8094\n",
      "Epoch 75/100\n",
      "1920/1920 [==============================] - 0s 127us/step - loss: 0.5758 - acc: 0.8094\n",
      "Epoch 76/100\n",
      "1920/1920 [==============================] - 0s 125us/step - loss: 0.5742 - acc: 0.8094\n",
      "Epoch 77/100\n",
      "1920/1920 [==============================] - 0s 128us/step - loss: 0.5737 - acc: 0.8094\n",
      "Epoch 78/100\n",
      "1920/1920 [==============================] - 0s 130us/step - loss: 0.5719 - acc: 0.8094\n",
      "Epoch 79/100\n",
      "1920/1920 [==============================] - 0s 131us/step - loss: 0.5711 - acc: 0.8083\n",
      "Epoch 80/100\n",
      "1920/1920 [==============================] - 0s 138us/step - loss: 0.5696 - acc: 0.8099\n",
      "Epoch 81/100\n",
      "1920/1920 [==============================] - 0s 120us/step - loss: 0.5683 - acc: 0.8172\n",
      "Epoch 82/100\n",
      "1920/1920 [==============================] - 0s 121us/step - loss: 0.5674 - acc: 0.8208\n",
      "Epoch 83/100\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.5660 - acc: 0.8208\n",
      "Epoch 84/100\n",
      "1920/1920 [==============================] - 0s 113us/step - loss: 0.5653 - acc: 0.8198\n",
      "Epoch 85/100\n",
      "1920/1920 [==============================] - 0s 108us/step - loss: 0.5635 - acc: 0.8214 0s - loss: 0.5625 - acc: 0.821\n",
      "Epoch 86/100\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5623 - acc: 0.8208\n",
      "Epoch 87/100\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5612 - acc: 0.8208\n",
      "Epoch 88/100\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.5597 - acc: 0.8208\n",
      "Epoch 89/100\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.5584 - acc: 0.8214\n",
      "Epoch 90/100\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5576 - acc: 0.8203\n",
      "Epoch 91/100\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5564 - acc: 0.8208\n",
      "Epoch 92/100\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5554 - acc: 0.8214\n",
      "Epoch 93/100\n",
      "1920/1920 [==============================] - 0s 114us/step - loss: 0.5544 - acc: 0.8219\n",
      "Epoch 94/100\n",
      "1920/1920 [==============================] - 0s 112us/step - loss: 0.5530 - acc: 0.8229\n",
      "Epoch 95/100\n",
      "1920/1920 [==============================] - 0s 128us/step - loss: 0.5522 - acc: 0.8234\n",
      "Epoch 96/100\n",
      "1920/1920 [==============================] - 0s 129us/step - loss: 0.5512 - acc: 0.8234\n",
      "Epoch 97/100\n",
      "1920/1920 [==============================] - 0s 127us/step - loss: 0.5506 - acc: 0.8229\n",
      "Epoch 98/100\n",
      "1920/1920 [==============================] - 0s 122us/step - loss: 0.5496 - acc: 0.8240\n",
      "Epoch 99/100\n",
      "1920/1920 [==============================] - 0s 130us/step - loss: 0.5489 - acc: 0.8229\n",
      "Epoch 100/100\n",
      "1920/1920 [==============================] - 0s 121us/step - loss: 0.5484 - acc: 0.8229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17513780>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 0s 356us/step\n",
      "Test loss: 0.61411759456\n",
      "Test accuracy: 0.820833333333\n"
     ]
    }
   ],
   "source": [
    "score = classifier.evaluate(X_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing on made-up data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake=['This umbrella sucks', 'I really like this! Great', 'Amazing, except for the handle', 'I wish this were larger', 'Works great on rainy days but it\\'s too bulky', 'Never fails. EVER', 'adfhasdfjlasdklfjalsd;fkalskfjas;dkfjasl;']\n",
    "fake_vector=vectorizer.transform(fake)\n",
    "fake_matrix=fake_vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01485564,  0.01612477,  0.01245042,  0.08732177,  0.86924744],\n",
       "       [ 0.04533625,  0.03064492,  0.02977685,  0.14688113,  0.74736083],\n",
       "       [ 0.06426573,  0.03704768,  0.03891603,  0.17027591,  0.68949461],\n",
       "       [ 0.07903251,  0.04129742,  0.04553317,  0.18487364,  0.64926326],\n",
       "       [ 0.01769844,  0.01791328,  0.01432108,  0.09516492,  0.85490227],\n",
       "       [ 0.21544071,  0.06371476,  0.09276783,  0.24571601,  0.38236073],\n",
       "       [ 0.0771531 ,  0.04079749,  0.04472519,  0.18318509,  0.6541391 ]], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_predictions=classifier.predict(fake_matrix)\n",
    "fake_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Unfortunately it seems that our classifier is biased too much on the positive reviews. Now we try oversampling using SMOTE.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3360618577f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m sm=SMOTE(random_state=1, ratio='auto', k_neighbors=5, m_neighbors=10, \n\u001b[0;32m      4\u001b[0m          out_step=0.5, kind='regular', n_jobs=-1)\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Star Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mX_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm=SMOTE(random_state=1, ratio='auto', k_neighbors=5, m_neighbors=10, \n",
    "         out_step=0.5, kind='regular', n_jobs=-1)\n",
    "y=dataset['Star Rating']\n",
    "X_res, y_res= sm.fit_sample(X, y)\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(y_res)\n",
    "y_res=encoder.transform(y_res)\n",
    "y_res=np_utils.to_categorical(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd637b8>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classifier= KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=10, verbose=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 0)\n",
    "new_classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake=['This umbrella sucks', 'I really like this! Great', 'Amazing, except for the handle', 'I wish this were larger', 'Works great on rainy days but it\\'s too bulky', 'Never fails. EVER', 'adfhasdfjlasdklfjalsd;fkalskfjas;dkfjasl;']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_predictions_2=new_classifier.predict(fake_matrix)\n",
    "fake_predictions_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 67.22% (4.01%)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "results = cross_val_score(new_classifier, X_res, y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running same model more times on same data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1920/1920 [==============================] - 0s 118us/step - loss: 0.5431 - acc: 0.8229\n",
      "Epoch 2/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5424 - acc: 0.8224\n",
      "Epoch 3/200\n",
      "1920/1920 [==============================] - 0s 99us/step - loss: 0.5424 - acc: 0.8234\n",
      "Epoch 4/200\n",
      "1920/1920 [==============================] - 0s 99us/step - loss: 0.5420 - acc: 0.8229\n",
      "Epoch 5/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5419 - acc: 0.8245\n",
      "Epoch 6/200\n",
      "1920/1920 [==============================] - 0s 127us/step - loss: 0.5413 - acc: 0.8240\n",
      "Epoch 7/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5414 - acc: 0.8224\n",
      "Epoch 8/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5408 - acc: 0.8245\n",
      "Epoch 9/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5416 - acc: 0.8240\n",
      "Epoch 10/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5412 - acc: 0.8250\n",
      "Epoch 11/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5404 - acc: 0.8245\n",
      "Epoch 12/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5406 - acc: 0.8245\n",
      "Epoch 13/200\n",
      "1920/1920 [==============================] - 0s 119us/step - loss: 0.5404 - acc: 0.8229\n",
      "Epoch 14/200\n",
      "1920/1920 [==============================] - 0s 116us/step - loss: 0.5397 - acc: 0.8234\n",
      "Epoch 15/200\n",
      "1920/1920 [==============================] - 0s 115us/step - loss: 0.5393 - acc: 0.8234\n",
      "Epoch 16/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5403 - acc: 0.8229\n",
      "Epoch 17/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5394 - acc: 0.8229\n",
      "Epoch 18/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5391 - acc: 0.8234\n",
      "Epoch 19/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5389 - acc: 0.8214\n",
      "Epoch 20/200\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.5387 - acc: 0.8234\n",
      "Epoch 21/200\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5391 - acc: 0.8229\n",
      "Epoch 22/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5382 - acc: 0.8208\n",
      "Epoch 23/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5380 - acc: 0.8234\n",
      "Epoch 24/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5385 - acc: 0.8234\n",
      "Epoch 25/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5376 - acc: 0.8234\n",
      "Epoch 26/200\n",
      "1920/1920 [==============================] - 0s 122us/step - loss: 0.5387 - acc: 0.8219\n",
      "Epoch 27/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5375 - acc: 0.8224\n",
      "Epoch 28/200\n",
      "1920/1920 [==============================] - 0s 123us/step - loss: 0.5370 - acc: 0.8224\n",
      "Epoch 29/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5375 - acc: 0.8214\n",
      "Epoch 30/200\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5367 - acc: 0.8208\n",
      "Epoch 31/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5373 - acc: 0.8224\n",
      "Epoch 32/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5369 - acc: 0.8214\n",
      "Epoch 33/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5362 - acc: 0.8203\n",
      "Epoch 34/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5363 - acc: 0.8208\n",
      "Epoch 35/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5360 - acc: 0.8203\n",
      "Epoch 36/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5367 - acc: 0.8219\n",
      "Epoch 37/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5363 - acc: 0.8198\n",
      "Epoch 38/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5358 - acc: 0.8214\n",
      "Epoch 39/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5363 - acc: 0.8214\n",
      "Epoch 40/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5356 - acc: 0.8224\n",
      "Epoch 41/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5357 - acc: 0.8203\n",
      "Epoch 42/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5356 - acc: 0.8198\n",
      "Epoch 43/200\n",
      "1920/1920 [==============================] - 0s 108us/step - loss: 0.5355 - acc: 0.8198\n",
      "Epoch 44/200\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5347 - acc: 0.8208\n",
      "Epoch 45/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5351 - acc: 0.8214\n",
      "Epoch 46/200\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5346 - acc: 0.8203\n",
      "Epoch 47/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5347 - acc: 0.8198\n",
      "Epoch 48/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5349 - acc: 0.8198\n",
      "Epoch 49/200\n",
      "1920/1920 [==============================] - 0s 113us/step - loss: 0.5345 - acc: 0.8203\n",
      "Epoch 50/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5346 - acc: 0.8203\n",
      "Epoch 51/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5343 - acc: 0.8198\n",
      "Epoch 52/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5343 - acc: 0.8203\n",
      "Epoch 53/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5340 - acc: 0.8198\n",
      "Epoch 54/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5336 - acc: 0.8193\n",
      "Epoch 55/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5334 - acc: 0.8203\n",
      "Epoch 56/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5324 - acc: 0.8187\n",
      "Epoch 57/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5337 - acc: 0.8193\n",
      "Epoch 58/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5328 - acc: 0.8193\n",
      "Epoch 59/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5328 - acc: 0.8193\n",
      "Epoch 60/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5327 - acc: 0.8198\n",
      "Epoch 61/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5322 - acc: 0.8193\n",
      "Epoch 62/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5324 - acc: 0.8177\n",
      "Epoch 63/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5321 - acc: 0.8203\n",
      "Epoch 64/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5321 - acc: 0.8193\n",
      "Epoch 65/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5318 - acc: 0.8203\n",
      "Epoch 66/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5322 - acc: 0.8203\n",
      "Epoch 67/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5314 - acc: 0.8187\n",
      "Epoch 68/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5311 - acc: 0.8203\n",
      "Epoch 69/200\n",
      "1920/1920 [==============================] - 0s 113us/step - loss: 0.5313 - acc: 0.8203\n",
      "Epoch 70/200\n",
      "1920/1920 [==============================] - 0s 108us/step - loss: 0.5312 - acc: 0.8203\n",
      "Epoch 71/200\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.5308 - acc: 0.8203 0s - loss: 0.5288 - acc: 0.823\n",
      "Epoch 72/200\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5304 - acc: 0.8203\n",
      "Epoch 73/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5306 - acc: 0.8203\n",
      "Epoch 74/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5305 - acc: 0.8203\n",
      "Epoch 75/200\n",
      "1920/1920 [==============================] - 0s 111us/step - loss: 0.5309 - acc: 0.8203\n",
      "Epoch 76/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5298 - acc: 0.8208\n",
      "Epoch 77/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5304 - acc: 0.8203\n",
      "Epoch 78/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5298 - acc: 0.8203\n",
      "Epoch 79/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5295 - acc: 0.8203\n",
      "Epoch 80/200\n",
      "1920/1920 [==============================] - 0s 108us/step - loss: 0.5292 - acc: 0.8208\n",
      "Epoch 81/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5298 - acc: 0.8208\n",
      "Epoch 82/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5296 - acc: 0.8208\n",
      "Epoch 83/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5296 - acc: 0.8208\n",
      "Epoch 84/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5289 - acc: 0.8208\n",
      "Epoch 85/200\n",
      "1920/1920 [==============================] - 0s 99us/step - loss: 0.5290 - acc: 0.8208\n",
      "Epoch 86/200\n",
      "1920/1920 [==============================] - 0s 99us/step - loss: 0.5285 - acc: 0.8198\n",
      "Epoch 87/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5285 - acc: 0.8208\n",
      "Epoch 88/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5283 - acc: 0.8208\n",
      "Epoch 89/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5283 - acc: 0.8208\n",
      "Epoch 90/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5282 - acc: 0.8208\n",
      "Epoch 91/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5283 - acc: 0.8203\n",
      "Epoch 92/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5283 - acc: 0.8208\n",
      "Epoch 93/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5278 - acc: 0.8198\n",
      "Epoch 94/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5273 - acc: 0.8208\n",
      "Epoch 95/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5277 - acc: 0.8208\n",
      "Epoch 96/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5273 - acc: 0.8203\n",
      "Epoch 97/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5267 - acc: 0.8198\n",
      "Epoch 98/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5269 - acc: 0.8203\n",
      "Epoch 99/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5269 - acc: 0.8208\n",
      "Epoch 100/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5266 - acc: 0.8203\n",
      "Epoch 101/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5263 - acc: 0.8208\n",
      "Epoch 102/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5265 - acc: 0.8208\n",
      "Epoch 103/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5267 - acc: 0.8208\n",
      "Epoch 104/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5261 - acc: 0.8208\n",
      "Epoch 105/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5263 - acc: 0.8203\n",
      "Epoch 106/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5256 - acc: 0.8208\n",
      "Epoch 107/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5255 - acc: 0.8203\n",
      "Epoch 108/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5254 - acc: 0.8198\n",
      "Epoch 109/200\n",
      "1920/1920 [==============================] - 0s 108us/step - loss: 0.5254 - acc: 0.8208\n",
      "Epoch 110/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5246 - acc: 0.8208\n",
      "Epoch 111/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5251 - acc: 0.8214\n",
      "Epoch 112/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5249 - acc: 0.8203\n",
      "Epoch 113/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5249 - acc: 0.8198\n",
      "Epoch 114/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5246 - acc: 0.8198\n",
      "Epoch 115/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5244 - acc: 0.8193\n",
      "Epoch 116/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5246 - acc: 0.8203\n",
      "Epoch 117/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5243 - acc: 0.8182\n",
      "Epoch 118/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5242 - acc: 0.8203\n",
      "Epoch 119/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5240 - acc: 0.8193\n",
      "Epoch 120/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5241 - acc: 0.8198\n",
      "Epoch 121/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5236 - acc: 0.8198\n",
      "Epoch 122/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5237 - acc: 0.8203\n",
      "Epoch 123/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5237 - acc: 0.8203\n",
      "Epoch 124/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5242 - acc: 0.8203\n",
      "Epoch 125/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5224 - acc: 0.8193\n",
      "Epoch 126/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5235 - acc: 0.8187\n",
      "Epoch 127/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5230 - acc: 0.8198\n",
      "Epoch 128/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5226 - acc: 0.8198\n",
      "Epoch 129/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5226 - acc: 0.8187\n",
      "Epoch 130/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5231 - acc: 0.8193\n",
      "Epoch 131/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5227 - acc: 0.8203\n",
      "Epoch 132/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5225 - acc: 0.8193\n",
      "Epoch 133/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5225 - acc: 0.8203\n",
      "Epoch 134/200\n",
      "1920/1920 [==============================] - 0s 110us/step - loss: 0.5222 - acc: 0.8203\n",
      "Epoch 135/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5219 - acc: 0.8182\n",
      "Epoch 136/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5221 - acc: 0.8187\n",
      "Epoch 137/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5222 - acc: 0.8193\n",
      "Epoch 138/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5220 - acc: 0.8193\n",
      "Epoch 139/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5220 - acc: 0.8198\n",
      "Epoch 140/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5216 - acc: 0.8193\n",
      "Epoch 141/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5215 - acc: 0.8198\n",
      "Epoch 142/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5210 - acc: 0.8193\n",
      "Epoch 143/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5221 - acc: 0.8182\n",
      "Epoch 144/200\n",
      "1920/1920 [==============================] - 0s 108us/step - loss: 0.5207 - acc: 0.8193\n",
      "Epoch 145/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5216 - acc: 0.8193\n",
      "Epoch 146/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5203 - acc: 0.8193\n",
      "Epoch 147/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5206 - acc: 0.8193\n",
      "Epoch 148/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5210 - acc: 0.8193\n",
      "Epoch 149/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5208 - acc: 0.8193\n",
      "Epoch 150/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5208 - acc: 0.8203\n",
      "Epoch 151/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5204 - acc: 0.8182\n",
      "Epoch 152/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5205 - acc: 0.8182\n",
      "Epoch 153/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5197 - acc: 0.8193\n",
      "Epoch 154/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5202 - acc: 0.8193\n",
      "Epoch 155/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5199 - acc: 0.8193\n",
      "Epoch 156/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5195 - acc: 0.8203\n",
      "Epoch 157/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5197 - acc: 0.8187\n",
      "Epoch 158/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5200 - acc: 0.8187\n",
      "Epoch 159/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5196 - acc: 0.8193\n",
      "Epoch 160/200\n",
      "1920/1920 [==============================] - 0s 109us/step - loss: 0.5197 - acc: 0.8198\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5194 - acc: 0.8193\n",
      "Epoch 162/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5191 - acc: 0.8187\n",
      "Epoch 163/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5195 - acc: 0.8214\n",
      "Epoch 164/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5200 - acc: 0.8214\n",
      "Epoch 165/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5187 - acc: 0.8177\n",
      "Epoch 166/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5188 - acc: 0.8203\n",
      "Epoch 167/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5188 - acc: 0.8182\n",
      "Epoch 168/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5184 - acc: 0.8208\n",
      "Epoch 169/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5192 - acc: 0.8198\n",
      "Epoch 170/200\n",
      "1920/1920 [==============================] - 0s 107us/step - loss: 0.5188 - acc: 0.8219\n",
      "Epoch 171/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5188 - acc: 0.8182\n",
      "Epoch 172/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5189 - acc: 0.8208\n",
      "Epoch 173/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5188 - acc: 0.8203\n",
      "Epoch 174/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5187 - acc: 0.8203\n",
      "Epoch 175/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5186 - acc: 0.8208\n",
      "Epoch 176/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5182 - acc: 0.8208\n",
      "Epoch 177/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5185 - acc: 0.8203\n",
      "Epoch 178/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5188 - acc: 0.8214\n",
      "Epoch 179/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5185 - acc: 0.8193\n",
      "Epoch 180/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5182 - acc: 0.8203\n",
      "Epoch 181/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5183 - acc: 0.8203\n",
      "Epoch 182/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5181 - acc: 0.8214\n",
      "Epoch 183/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5183 - acc: 0.8219\n",
      "Epoch 184/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5180 - acc: 0.8214\n",
      "Epoch 185/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5173 - acc: 0.8214\n",
      "Epoch 186/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5178 - acc: 0.8203\n",
      "Epoch 187/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5178 - acc: 0.8214\n",
      "Epoch 188/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5175 - acc: 0.8203\n",
      "Epoch 189/200\n",
      "1920/1920 [==============================] - 0s 101us/step - loss: 0.5175 - acc: 0.8193\n",
      "Epoch 190/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5178 - acc: 0.8198\n",
      "Epoch 191/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5171 - acc: 0.8208\n",
      "Epoch 192/200\n",
      "1920/1920 [==============================] - 0s 105us/step - loss: 0.5167 - acc: 0.8208\n",
      "Epoch 193/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5169 - acc: 0.8214\n",
      "Epoch 194/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5166 - acc: 0.8208\n",
      "Epoch 195/200\n",
      "1920/1920 [==============================] - 0s 106us/step - loss: 0.5165 - acc: 0.8203\n",
      "Epoch 196/200\n",
      "1920/1920 [==============================] - 0s 103us/step - loss: 0.5166 - acc: 0.8208\n",
      "Epoch 197/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5162 - acc: 0.8208\n",
      "Epoch 198/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5164 - acc: 0.8198\n",
      "Epoch 199/200\n",
      "1920/1920 [==============================] - 0s 102us/step - loss: 0.5156 - acc: 0.8208\n",
      "Epoch 200/200\n",
      "1920/1920 [==============================] - 0s 104us/step - loss: 0.5158 - acc: 0.8203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf493c88>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>This results in a worse accuracy (81%)\n",
    "    \n",
    "To get better results we need more data and possibly a component from emotional analysis. </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K-Fold</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 83.67% (2.38%)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "results = cross_val_score(classifier, X, y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Saving model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = classifier.to_json()\n",
    "with open(\"AmazonRatingANN.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"AmazonRatingANN.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Unanswered questions</h1>\n",
    "1. How to choose the right hash size\n",
    "2. How to actually use this in a business problem\n",
    "3. How to get emotions for each input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SOM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to load fast implementation of SMLR.  May be you forgotten to build it.  We will use much slower pure-Python version. Original exception was [WinError 126] The specified module could not be found\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\scipy\\integrate\\quadpack.py:364: IntegrationWarning: Extremely bad integrand behavior occurs at some points of the\n",
      "  integration interval.\n",
      "  warnings.warn(msg, IntegrationWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: SMLR: C implementation is not available. Using pure Python one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\mvpa2\\datasets\\sources\\skl_data.py:32: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  argnames, varargs, varkw, defaults = inspect.getargspec(fx)\n",
      "<string>:60: DeprecationWarning: invalid escape sequence \\#\n",
      "<string>:59: DeprecationWarning: invalid escape sequence \\#\n",
      "<string>:59: DeprecationWarning: invalid escape sequence \\#\n",
      "<string>:65: DeprecationWarning: invalid escape sequence \\c\n"
     ]
    }
   ],
   "source": [
    "from mvpa2.suite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arr=vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = SimpleSOMMapper((20, 30), 400, learning_rate=0.05)\n",
    "som.train(X_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -1.63231430e-01,  -2.12023976e-02,   2.64967467e-01, ...,\n",
       "           1.08223717e-01,   2.02714840e-01,   5.39986404e-02],\n",
       "        [ -1.04001559e-01,   3.76143380e-02,   3.80886612e-01, ...,\n",
       "           6.67869201e-02,   7.71265617e-02,   6.63713425e-02],\n",
       "        [ -2.00105897e-02,   7.89084503e-03,   5.86958105e-01, ...,\n",
       "          -7.00586829e-04,  -2.96551813e-02,   1.64379571e-01],\n",
       "        ..., \n",
       "        [ -2.47743671e-02,  -6.10222786e-01,   9.02851233e-02, ...,\n",
       "          -1.62993410e-02,   4.62192227e-01,  -4.84200686e-02],\n",
       "        [ -3.73261346e-02,  -3.81609517e-01,   2.13799608e-01, ...,\n",
       "          -2.56572293e-02,   2.03346929e-01,  -6.22650067e-02],\n",
       "        [ -1.25713012e-01,  -1.57983132e-01,   2.03186833e-01, ...,\n",
       "           8.70507170e-02,   2.02088132e-01,   9.80057493e-03]],\n",
       "\n",
       "       [[ -1.54760709e-01,  -6.79466693e-03,   3.08744127e-01, ...,\n",
       "           1.34084508e-01,   3.59844832e-01,   6.13661604e-02],\n",
       "        [ -5.68673121e-02,   8.23532871e-02,   5.50610991e-01, ...,\n",
       "           5.92299943e-02,   1.76184904e-01,   1.63987228e-01],\n",
       "        [ -1.12585125e-02,   4.41566696e-02,   6.48585913e-01, ...,\n",
       "           1.66774192e-02,   1.43768424e-02,   2.09051160e-01],\n",
       "        ..., \n",
       "        [ -2.29764278e-03,  -6.67413719e-01,   6.89152496e-03, ...,\n",
       "           4.82190950e-03,   6.75943939e-01,  -6.29867218e-03],\n",
       "        [ -2.63123511e-02,  -5.53558311e-01,   6.86495249e-03, ...,\n",
       "           3.57849564e-02,   6.13576114e-01,  -4.69171760e-02],\n",
       "        [ -1.40825973e-01,  -1.06473275e-01,   1.99368605e-01, ...,\n",
       "           9.82991833e-02,   4.33346000e-01,  -5.71971542e-03]],\n",
       "\n",
       "       [[ -9.46531368e-02,   4.53044214e-02,   3.77961292e-01, ...,\n",
       "           5.78154289e-02,   4.68128087e-01,   1.49541177e-02],\n",
       "        [ -2.49051043e-02,   1.39192452e-01,   5.27932599e-01, ...,\n",
       "           3.70385154e-02,   3.47647520e-01,   1.15524587e-01],\n",
       "        [  2.99879582e-02,   8.12931107e-02,   6.57310452e-01, ...,\n",
       "           4.18152863e-02,   1.02455191e-01,   1.22556689e-01],\n",
       "        ..., \n",
       "        [ -6.39901234e-03,  -5.83284510e-01,  -4.76242442e-03, ...,\n",
       "           2.66028259e-02,   6.54490306e-01,  -2.37251007e-02],\n",
       "        [ -6.59563081e-02,  -3.15119076e-01,   5.42906458e-02, ...,\n",
       "           3.19947227e-02,   6.20318805e-01,  -6.16689003e-02],\n",
       "        [ -9.82316780e-02,  -1.46137266e-01,   1.83226350e-01, ...,\n",
       "           6.12158012e-02,   5.52290931e-01,  -5.31831710e-02]],\n",
       "\n",
       "       ..., \n",
       "       [[  1.95796361e-01,   7.87909256e-02,  -7.21544560e-02, ...,\n",
       "          -2.23850658e-02,  -3.21719856e-02,  -1.15063748e-01],\n",
       "        [  1.09674874e-01,   9.65320935e-02,  -4.61943872e-02, ...,\n",
       "          -2.29987865e-02,  -5.65622049e-02,   4.32464428e-02],\n",
       "        [  1.00810831e-01,   8.38562560e-02,  -3.97071325e-02, ...,\n",
       "          -2.70578969e-02,  -2.69886753e-02,   1.15822891e-01],\n",
       "        ..., \n",
       "        [  3.57057514e-01,  -2.71647656e-01,   2.62162421e-02, ...,\n",
       "          -6.22127756e-02,   5.23209864e-02,  -5.55933058e-03],\n",
       "        [  3.07744343e-01,  -3.68455663e-03,  -1.85428870e-02, ...,\n",
       "           5.22634508e-03,   2.30004541e-02,  -1.32937374e-01],\n",
       "        [  2.91028412e-01,   7.59799874e-02,  -1.91388042e-02, ...,\n",
       "           1.53067964e-02,  -2.99310395e-02,  -1.56750532e-01]],\n",
       "\n",
       "       [[  8.34648131e-02,   6.93969673e-02,   6.25230221e-02, ...,\n",
       "           5.66930056e-03,  -3.12799160e-02,  -7.96512299e-02],\n",
       "        [  6.30043881e-02,   5.86967061e-02,   6.64109460e-02, ...,\n",
       "          -1.98994888e-02,  -4.23790957e-03,  -1.51118749e-02],\n",
       "        [  2.27092391e-02,   1.41693507e-02,   9.26217253e-02, ...,\n",
       "          -5.57944717e-03,  -1.99215482e-02,   6.79325935e-02],\n",
       "        ..., \n",
       "        [  1.58906280e-01,  -3.79320903e-01,   2.32851454e-01, ...,\n",
       "          -2.27149014e-02,  -1.68718046e-02,  -3.99751645e-02],\n",
       "        [  1.75713975e-01,  -1.47619029e-01,   2.04546203e-01, ...,\n",
       "          -9.73298744e-03,  -2.21959333e-03,  -5.47109488e-02],\n",
       "        [  1.34049869e-01,   3.96033216e-03,   9.56607758e-02, ...,\n",
       "           8.75297006e-03,  -1.03895265e-02,  -1.35055122e-01]],\n",
       "\n",
       "       [[ -5.61503873e-02,  -2.86119752e-04,   2.32999455e-01, ...,\n",
       "           4.80054019e-02,   6.20509106e-02,   9.21651015e-03],\n",
       "        [ -3.20642740e-02,   2.13009002e-02,   2.45343765e-01, ...,\n",
       "           1.36587220e-02,   9.35637198e-03,   1.37026795e-03],\n",
       "        [ -2.51466275e-02,  -2.13827850e-02,   3.09607729e-01, ...,\n",
       "           1.31098046e-03,  -3.57732184e-02,   2.94490802e-02],\n",
       "        ..., \n",
       "        [  3.16794426e-02,  -3.87283722e-01,   3.05226915e-01, ...,\n",
       "          -1.40637085e-02,  -1.71846071e-03,  -3.55072112e-02],\n",
       "        [  2.98337717e-03,  -2.92258829e-01,   3.33835710e-01, ...,\n",
       "          -4.53431929e-02,   2.47207305e-02,  -6.13017564e-02],\n",
       "        [ -3.58692210e-02,  -1.49761340e-01,   2.92864799e-01, ...,\n",
       "          -3.93133694e-03,   3.85006873e-02,  -1.07602892e-02]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "som.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sammon(x, n = 2, display = 2, inputdist = 'raw', maxhalves = 20, maxiter = 500, tolfun = 1e-9, init = 'pca'):\n",
    "\n",
    "    import numpy as np \n",
    "    from scipy.spatial.distance import cdist\n",
    "\n",
    "    \"\"\"Perform Sammon mapping on dataset x\n",
    "\n",
    "    y = sammon(x) applies the Sammon nonlinear mapping procedure on\n",
    "    multivariate data x, where each row represents a pattern and each column\n",
    "    represents a feature.  On completion, y contains the corresponding\n",
    "    co-ordinates of each point on the map.  By default, a two-dimensional\n",
    "    map is created.  Note if x contains any duplicated rows, SAMMON will\n",
    "    fail (ungracefully). \n",
    "\n",
    "    [y,E] = sammon(x) also returns the value of the cost function in E (i.e.\n",
    "    the stress of the mapping).\n",
    "\n",
    "    An N-dimensional output map is generated by y = sammon(x,n) .\n",
    "\n",
    "    A set of optimisation options can be specified using optional\n",
    "    arguments, y = sammon(x,n,[OPTS]):\n",
    "\n",
    "       maxiter        - maximum number of iterations\n",
    "       tolfun         - relative tolerance on objective function\n",
    "       maxhalves      - maximum number of step halvings\n",
    "       input          - {'raw','distance'} if set to 'distance', X is \n",
    "                        interpreted as a matrix of pairwise distances.\n",
    "       display        - 0 to 2. 0 least verbose, 2 max verbose.\n",
    "       init           - {'pca', 'random'}\n",
    "\n",
    "    The default options are retrieved by calling sammon(x) with no\n",
    "    parameters.\n",
    "\n",
    "    File        : sammon.py\n",
    "    Date        : 18 April 2014\n",
    "    Authors     : Tom J. Pollard (tom.pollard.11@ucl.ac.uk)\n",
    "                : Ported from MATLAB implementation by \n",
    "                  Gavin C. Cawley and Nicola L. C. Talbot\n",
    "\n",
    "    Description : Simple python implementation of Sammon's non-linear\n",
    "                  mapping algorithm [1].\n",
    "\n",
    "    References  : [1] Sammon, John W. Jr., \"A Nonlinear Mapping for Data\n",
    "                  Structure Analysis\", IEEE Transactions on Computers,\n",
    "                  vol. C-18, no. 5, pp 401-409, May 1969.\n",
    "\n",
    "    Copyright   : (c) Dr Gavin C. Cawley, November 2007.\n",
    "\n",
    "    This program is free software; you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation; either version 2 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program; if not, write to the Free Software\n",
    "    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n",
    "\n",
    "    \"\"\"\n",
    "#\n",
    "#    def euclid(a,b):\n",
    "#        d = np.sqrt( ((a**2).sum(axis=1)*np.ones([1,b.shape[0]]).T).T + \\\n",
    "#            np.ones([a.shape[0],1])*(b**2).sum(axis=1)-2*(np.dot(a,b.T)))\n",
    "#        return d\n",
    "\n",
    "    X = x\n",
    "\n",
    "    # Create distance matrix unless given by parameters\n",
    "    if inputdist == 'distance':\n",
    "        xD = X\n",
    "    else:\n",
    "        xD = cdist(X, X)\n",
    "\n",
    "    # Remaining initialisation\n",
    "    N = X.shape[0] # hmmm, shape[1]?\n",
    "    scale = 0.5 / xD.sum()\n",
    "\n",
    "    if init == 'pca':\n",
    "        [UU,DD,_] = np.linalg.svd(X)\n",
    "        Y = UU[:,:n]*DD[:n] \n",
    "    else:\n",
    "        Y = np.random.normal(0.0,1.0,[N,n])\n",
    "    one = np.ones([N,n])\n",
    "\n",
    "    xD = xD + np.eye(N)        \n",
    "    xDinv = 1 / xD # Returns inf where D = 0.\n",
    "    xDinv[np.isinf(xDinv)] = 0 # Fix by replacing inf with 0 (default Matlab behaviour).    \n",
    "    yD = cdist(Y, Y) + np.eye(N)\n",
    "    yDinv = 1. / yD # Returns inf where d = 0. \n",
    "    \n",
    "    np.fill_diagonal(xD, 1)    \n",
    "    np.fill_diagonal(yD, 1)\n",
    "    np.fill_diagonal(xDinv, 0)\n",
    "    np.fill_diagonal(yDinv, 0)\n",
    "    \n",
    "    xDinv[np.isnan(xDinv)] = 0\n",
    "    yDinv[np.isnan(xDinv)] = 0\n",
    "    xDinv[np.isinf(xDinv)] = 0    \n",
    "    yDinv[np.isinf(yDinv)] = 0 # Fix by replacing inf with 0 (default Matlab behaviour).\n",
    "    \n",
    "    delta = xD - yD \n",
    "    E = ((delta**2)*xDinv).sum() \n",
    "\n",
    "    # Get on with it\n",
    "    for i in range(maxiter):\n",
    "\n",
    "        # Compute gradient, Hessian and search direction (note it is actually\n",
    "        # 1/4 of the gradient and Hessian, but the step size is just the ratio\n",
    "        # of the gradient and the diagonal of the Hessian so it doesn't\n",
    "        # matter).\n",
    "        delta = yDinv - xDinv\n",
    "        deltaone = np.dot(delta,one)\n",
    "        g = np.dot(delta, Y) - (Y * deltaone)\n",
    "        dinv3 = yDinv ** 3\n",
    "        y2 = Y ** 2\n",
    "        H = np.dot(dinv3,y2) - deltaone - np.dot(2, Y) * np.dot(dinv3, Y) + y2 * np.dot(dinv3,one)\n",
    "        s = -g.flatten(order='F') / np.abs(H.flatten(order='F'))\n",
    "        y_old = Y\n",
    "\n",
    "        # Use step-halving procedure to ensure progress is made\n",
    "        for j in range(maxhalves):\n",
    "            s_reshape = s.reshape(2,len(s)//2).T \n",
    "            #s_reshape = s.reshape(-1).T\n",
    "            y = y_old + s_reshape\n",
    "            d = cdist(y, y) + np.eye(N)\n",
    "            dinv = 1 / d # Returns inf where D = 0. \n",
    "            dinv[np.isinf(dinv)] = 0 # Fix by replacing inf with 0 (default Matlab behaviour).\n",
    "            delta = xD - d\n",
    "            E_new = ((delta**2)*xDinv).sum()\n",
    "            if E_new < E:\n",
    "                break\n",
    "            else:\n",
    "                s = np.dot(0.5,s)\n",
    "\n",
    "        # Bomb out if too many halving steps are required\n",
    "        if j == maxhalves:\n",
    "            print('Warning: maxhalves exceeded. Sammon mapping may not converge...')\n",
    "\n",
    "        # Evaluate termination criterion\n",
    "        if np.abs((E - E_new) / E) < tolfun:\n",
    "            if display:\n",
    "                print('TolFun exceeded: Optimisation terminated')\n",
    "            break\n",
    "\n",
    "        # Report progress\n",
    "        E = E_new\n",
    "        if display > 1:\n",
    "            print('epoch = ' + str(i) + ': E = ' + str(E * scale))\n",
    "\n",
    "    # Fiddle stress to match the original Sammon paper\n",
    "    E = E * scale\n",
    "    \n",
    "    return [y,E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 30, 20)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "som.K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 20)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0: E = 0.284084295351\n",
      "epoch = 1: E = 0.282075886171\n",
      "epoch = 2: E = 0.284171160353\n",
      "epoch = 3: E = 0.284084295351\n",
      "epoch = 4: E = 0.282075886171\n",
      "epoch = 5: E = 0.284171160353\n",
      "epoch = 6: E = 0.284084295351\n",
      "epoch = 7: E = 0.282075886171\n",
      "epoch = 8: E = 0.284171160353\n",
      "epoch = 9: E = 0.284084295351\n",
      "epoch = 10: E = 0.282075886171\n",
      "epoch = 11: E = 0.284171160353\n",
      "epoch = 12: E = 0.284084295351\n",
      "epoch = 13: E = 0.282075886171\n",
      "epoch = 14: E = 0.284171160353\n",
      "epoch = 15: E = 0.284084295351\n",
      "epoch = 16: E = 0.282075886171\n",
      "epoch = 17: E = 0.284171160353\n",
      "epoch = 18: E = 0.284084295351\n",
      "epoch = 19: E = 0.282075886171\n",
      "epoch = 20: E = 0.284171160353\n",
      "epoch = 21: E = 0.284084295351\n",
      "epoch = 22: E = 0.282075886171\n",
      "epoch = 23: E = 0.284171160353\n",
      "epoch = 24: E = 0.284084295351\n",
      "epoch = 25: E = 0.282075886171\n",
      "epoch = 26: E = 0.284171160353\n",
      "epoch = 27: E = 0.284084295351\n",
      "epoch = 28: E = 0.282075886171\n",
      "epoch = 29: E = 0.284171160353\n",
      "epoch = 30: E = 0.284084295351\n",
      "epoch = 31: E = 0.282075886171\n",
      "epoch = 32: E = 0.284171160353\n",
      "epoch = 33: E = 0.284084295351\n",
      "epoch = 34: E = 0.282075886171\n",
      "epoch = 35: E = 0.284171160353\n",
      "epoch = 36: E = 0.284084295351\n",
      "epoch = 37: E = 0.282075886171\n",
      "epoch = 38: E = 0.284171160353\n",
      "epoch = 39: E = 0.284084295351\n",
      "epoch = 40: E = 0.282075886171\n",
      "epoch = 41: E = 0.284171160353\n",
      "epoch = 42: E = 0.284084295351\n",
      "epoch = 43: E = 0.282075886171\n",
      "epoch = 44: E = 0.284171160353\n",
      "epoch = 45: E = 0.284084295351\n",
      "epoch = 46: E = 0.282075886171\n",
      "epoch = 47: E = 0.284171160353\n",
      "epoch = 48: E = 0.284084295351\n",
      "epoch = 49: E = 0.282075886171\n",
      "epoch = 50: E = 0.284171160353\n",
      "epoch = 51: E = 0.284084295351\n",
      "epoch = 52: E = 0.282075886171\n",
      "epoch = 53: E = 0.284171160353\n",
      "epoch = 54: E = 0.284084295351\n",
      "epoch = 55: E = 0.282075886171\n",
      "epoch = 56: E = 0.284171160353\n",
      "epoch = 57: E = 0.284084295351\n",
      "epoch = 58: E = 0.282075886171\n",
      "epoch = 59: E = 0.284171160353\n",
      "epoch = 60: E = 0.284084295351\n",
      "epoch = 61: E = 0.282075886171\n",
      "epoch = 62: E = 0.284171160353\n",
      "epoch = 63: E = 0.284084295351\n",
      "epoch = 64: E = 0.282075886171\n",
      "epoch = 65: E = 0.284171160353\n",
      "epoch = 66: E = 0.284084295351\n",
      "epoch = 67: E = 0.282075886171\n",
      "epoch = 68: E = 0.284171160353\n",
      "epoch = 69: E = 0.284084295351\n",
      "epoch = 70: E = 0.282075886171\n",
      "epoch = 71: E = 0.284171160353\n",
      "epoch = 72: E = 0.284084295351\n",
      "epoch = 73: E = 0.282075886171\n",
      "epoch = 74: E = 0.284171160353\n",
      "epoch = 75: E = 0.284084295351\n",
      "epoch = 76: E = 0.282075886171\n",
      "epoch = 77: E = 0.284171160353\n",
      "epoch = 78: E = 0.284084295351\n",
      "epoch = 79: E = 0.282075886171\n",
      "epoch = 80: E = 0.284171160353\n",
      "epoch = 81: E = 0.284084295351\n",
      "epoch = 82: E = 0.282075886171\n",
      "epoch = 83: E = 0.284171160353\n",
      "epoch = 84: E = 0.284084295351\n",
      "epoch = 85: E = 0.282075886171\n",
      "epoch = 86: E = 0.284171160353\n",
      "epoch = 87: E = 0.284084295351\n",
      "epoch = 88: E = 0.282075886171\n",
      "epoch = 89: E = 0.284171160353\n",
      "epoch = 90: E = 0.284084295351\n",
      "epoch = 91: E = 0.282075886171\n",
      "epoch = 92: E = 0.284171160353\n",
      "epoch = 93: E = 0.284084295351\n",
      "epoch = 94: E = 0.282075886171\n",
      "epoch = 95: E = 0.284171160353\n",
      "epoch = 96: E = 0.284084295351\n",
      "epoch = 97: E = 0.282075886171\n",
      "epoch = 98: E = 0.284171160353\n",
      "epoch = 99: E = 0.284084295351\n",
      "epoch = 100: E = 0.282075886171\n",
      "epoch = 101: E = 0.284171160353\n",
      "epoch = 102: E = 0.284084295351\n",
      "epoch = 103: E = 0.282075886171\n",
      "epoch = 104: E = 0.284171160353\n",
      "epoch = 105: E = 0.284084295351\n",
      "epoch = 106: E = 0.282075886171\n",
      "epoch = 107: E = 0.284171160353\n",
      "epoch = 108: E = 0.284084295351\n",
      "epoch = 109: E = 0.282075886171\n",
      "epoch = 110: E = 0.284171160353\n",
      "epoch = 111: E = 0.284084295351\n",
      "epoch = 112: E = 0.282075886171\n",
      "epoch = 113: E = 0.284171160353\n",
      "epoch = 114: E = 0.284084295351\n",
      "epoch = 115: E = 0.282075886171\n",
      "epoch = 116: E = 0.284171160353\n",
      "epoch = 117: E = 0.284084295351\n",
      "epoch = 118: E = 0.282075886171\n",
      "epoch = 119: E = 0.284171160353\n",
      "epoch = 120: E = 0.284084295351\n",
      "epoch = 121: E = 0.282075886171\n",
      "epoch = 122: E = 0.284171160353\n",
      "epoch = 123: E = 0.284084295351\n",
      "epoch = 124: E = 0.282075886171\n",
      "epoch = 125: E = 0.284171160353\n",
      "epoch = 126: E = 0.284084295351\n",
      "epoch = 127: E = 0.282075886171\n",
      "epoch = 128: E = 0.284171160353\n",
      "epoch = 129: E = 0.284084295351\n",
      "epoch = 130: E = 0.282075886171\n",
      "epoch = 131: E = 0.284171160353\n",
      "epoch = 132: E = 0.284084295351\n",
      "epoch = 133: E = 0.282075886171\n",
      "epoch = 134: E = 0.284171160353\n",
      "epoch = 135: E = 0.284084295351\n",
      "epoch = 136: E = 0.282075886171\n",
      "epoch = 137: E = 0.284171160353\n",
      "epoch = 138: E = 0.284084295351\n",
      "epoch = 139: E = 0.282075886171\n",
      "epoch = 140: E = 0.284171160353\n",
      "epoch = 141: E = 0.284084295351\n",
      "epoch = 142: E = 0.282075886171\n",
      "epoch = 143: E = 0.284171160353\n",
      "epoch = 144: E = 0.284084295351\n",
      "epoch = 145: E = 0.282075886171\n",
      "epoch = 146: E = 0.284171160353\n",
      "epoch = 147: E = 0.284084295351\n",
      "epoch = 148: E = 0.282075886171\n",
      "epoch = 149: E = 0.284171160353\n",
      "epoch = 150: E = 0.284084295351\n",
      "epoch = 151: E = 0.282075886171\n",
      "epoch = 152: E = 0.284171160353\n",
      "epoch = 153: E = 0.284084295351\n",
      "epoch = 154: E = 0.282075886171\n",
      "epoch = 155: E = 0.284171160353\n",
      "epoch = 156: E = 0.284084295351\n",
      "epoch = 157: E = 0.282075886171\n",
      "epoch = 158: E = 0.284171160353\n",
      "epoch = 159: E = 0.284084295351\n",
      "epoch = 160: E = 0.282075886171\n",
      "epoch = 161: E = 0.284171160353\n",
      "epoch = 162: E = 0.284084295351\n",
      "epoch = 163: E = 0.282075886171\n",
      "epoch = 164: E = 0.284171160353\n",
      "epoch = 165: E = 0.284084295351\n",
      "epoch = 166: E = 0.282075886171\n",
      "epoch = 167: E = 0.284171160353\n",
      "epoch = 168: E = 0.284084295351\n",
      "epoch = 169: E = 0.282075886171\n",
      "epoch = 170: E = 0.284171160353\n",
      "epoch = 171: E = 0.284084295351\n",
      "epoch = 172: E = 0.282075886171\n",
      "epoch = 173: E = 0.284171160353\n",
      "epoch = 174: E = 0.284084295351\n",
      "epoch = 175: E = 0.282075886171\n",
      "epoch = 176: E = 0.284171160353\n",
      "epoch = 177: E = 0.284084295351\n",
      "epoch = 178: E = 0.282075886171\n",
      "epoch = 179: E = 0.284171160353\n",
      "epoch = 180: E = 0.284084295351\n",
      "epoch = 181: E = 0.282075886171\n",
      "epoch = 182: E = 0.284171160353\n",
      "epoch = 183: E = 0.284084295351\n",
      "epoch = 184: E = 0.282075886171\n",
      "epoch = 185: E = 0.284171160353\n",
      "epoch = 186: E = 0.284084295351\n",
      "epoch = 187: E = 0.282075886171\n",
      "epoch = 188: E = 0.284171160353\n",
      "epoch = 189: E = 0.284084295351\n",
      "epoch = 190: E = 0.282075886171\n",
      "epoch = 191: E = 0.284171160353\n",
      "epoch = 192: E = 0.284084295351\n",
      "epoch = 193: E = 0.282075886171\n",
      "epoch = 194: E = 0.284171160353\n",
      "epoch = 195: E = 0.284084295351\n",
      "epoch = 196: E = 0.282075886171\n",
      "epoch = 197: E = 0.284171160353\n",
      "epoch = 198: E = 0.284084295351\n",
      "epoch = 199: E = 0.282075886171\n",
      "epoch = 200: E = 0.284171160353\n",
      "epoch = 201: E = 0.284084295351\n",
      "epoch = 202: E = 0.282075886171\n",
      "epoch = 203: E = 0.284171160353\n",
      "epoch = 204: E = 0.284084295351\n",
      "epoch = 205: E = 0.282075886171\n",
      "epoch = 206: E = 0.284171160353\n",
      "epoch = 207: E = 0.284084295351\n",
      "epoch = 208: E = 0.282075886171\n",
      "epoch = 209: E = 0.284171160353\n",
      "epoch = 210: E = 0.284084295351\n",
      "epoch = 211: E = 0.282075886171\n",
      "epoch = 212: E = 0.284171160353\n",
      "epoch = 213: E = 0.284084295351\n",
      "epoch = 214: E = 0.282075886171\n",
      "epoch = 215: E = 0.284171160353\n",
      "epoch = 216: E = 0.284084295351\n",
      "epoch = 217: E = 0.282075886171\n",
      "epoch = 218: E = 0.284171160353\n",
      "epoch = 219: E = 0.284084295351\n",
      "epoch = 220: E = 0.282075886171\n",
      "epoch = 221: E = 0.284171160353\n",
      "epoch = 222: E = 0.284084295351\n",
      "epoch = 223: E = 0.282075886171\n",
      "epoch = 224: E = 0.284171160353\n",
      "epoch = 225: E = 0.284084295351\n",
      "epoch = 226: E = 0.282075886171\n",
      "epoch = 227: E = 0.284171160353\n",
      "epoch = 228: E = 0.284084295351\n",
      "epoch = 229: E = 0.282075886171\n",
      "epoch = 230: E = 0.284171160353\n",
      "epoch = 231: E = 0.284084295351\n",
      "epoch = 232: E = 0.282075886171\n",
      "epoch = 233: E = 0.284171160353\n",
      "epoch = 234: E = 0.284084295351\n",
      "epoch = 235: E = 0.282075886171\n",
      "epoch = 236: E = 0.284171160353\n",
      "epoch = 237: E = 0.284084295351\n",
      "epoch = 238: E = 0.282075886171\n",
      "epoch = 239: E = 0.284171160353\n",
      "epoch = 240: E = 0.284084295351\n",
      "epoch = 241: E = 0.282075886171\n",
      "epoch = 242: E = 0.284171160353\n",
      "epoch = 243: E = 0.284084295351\n",
      "epoch = 244: E = 0.282075886171\n",
      "epoch = 245: E = 0.284171160353\n",
      "epoch = 246: E = 0.284084295351\n",
      "epoch = 247: E = 0.282075886171\n",
      "epoch = 248: E = 0.284171160353\n",
      "epoch = 249: E = 0.284084295351\n",
      "epoch = 250: E = 0.282075886171\n",
      "epoch = 251: E = 0.284171160353\n",
      "epoch = 252: E = 0.284084295351\n",
      "epoch = 253: E = 0.282075886171\n",
      "epoch = 254: E = 0.284171160353\n",
      "epoch = 255: E = 0.284084295351\n",
      "epoch = 256: E = 0.282075886171\n",
      "epoch = 257: E = 0.284171160353\n",
      "epoch = 258: E = 0.284084295351\n",
      "epoch = 259: E = 0.282075886171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 260: E = 0.284171160353\n",
      "epoch = 261: E = 0.284084295351\n",
      "epoch = 262: E = 0.282075886171\n",
      "epoch = 263: E = 0.284171160353\n",
      "epoch = 264: E = 0.284084295351\n",
      "epoch = 265: E = 0.282075886171\n",
      "epoch = 266: E = 0.284171160353\n",
      "epoch = 267: E = 0.284084295351\n",
      "epoch = 268: E = 0.282075886171\n",
      "epoch = 269: E = 0.284171160353\n",
      "epoch = 270: E = 0.284084295351\n",
      "epoch = 271: E = 0.282075886171\n",
      "epoch = 272: E = 0.284171160353\n",
      "epoch = 273: E = 0.284084295351\n",
      "epoch = 274: E = 0.282075886171\n",
      "epoch = 275: E = 0.284171160353\n",
      "epoch = 276: E = 0.284084295351\n",
      "epoch = 277: E = 0.282075886171\n",
      "epoch = 278: E = 0.284171160353\n",
      "epoch = 279: E = 0.284084295351\n",
      "epoch = 280: E = 0.282075886171\n",
      "epoch = 281: E = 0.284171160353\n",
      "epoch = 282: E = 0.284084295351\n",
      "epoch = 283: E = 0.282075886171\n",
      "epoch = 284: E = 0.284171160353\n",
      "epoch = 285: E = 0.284084295351\n",
      "epoch = 286: E = 0.282075886171\n",
      "epoch = 287: E = 0.284171160353\n",
      "epoch = 288: E = 0.284084295351\n",
      "epoch = 289: E = 0.282075886171\n",
      "epoch = 290: E = 0.284171160353\n",
      "epoch = 291: E = 0.284084295351\n",
      "epoch = 292: E = 0.282075886171\n",
      "epoch = 293: E = 0.284171160353\n",
      "epoch = 294: E = 0.284084295351\n",
      "epoch = 295: E = 0.282075886171\n",
      "epoch = 296: E = 0.284171160353\n",
      "epoch = 297: E = 0.284084295351\n",
      "epoch = 298: E = 0.282075886171\n",
      "epoch = 299: E = 0.284171160353\n",
      "epoch = 300: E = 0.284084295351\n",
      "epoch = 301: E = 0.282075886171\n",
      "epoch = 302: E = 0.284171160353\n",
      "epoch = 303: E = 0.284084295351\n",
      "epoch = 304: E = 0.282075886171\n",
      "epoch = 305: E = 0.284171160353\n",
      "epoch = 306: E = 0.284084295351\n",
      "epoch = 307: E = 0.282075886171\n",
      "epoch = 308: E = 0.284171160353\n",
      "epoch = 309: E = 0.284084295351\n",
      "epoch = 310: E = 0.282075886171\n",
      "epoch = 311: E = 0.284171160353\n",
      "epoch = 312: E = 0.284084295351\n",
      "epoch = 313: E = 0.282075886171\n",
      "epoch = 314: E = 0.284171160353\n",
      "epoch = 315: E = 0.284084295351\n",
      "epoch = 316: E = 0.282075886171\n",
      "epoch = 317: E = 0.284171160353\n",
      "epoch = 318: E = 0.284084295351\n",
      "epoch = 319: E = 0.282075886171\n",
      "epoch = 320: E = 0.284171160353\n",
      "epoch = 321: E = 0.284084295351\n",
      "epoch = 322: E = 0.282075886171\n",
      "epoch = 323: E = 0.284171160353\n",
      "epoch = 324: E = 0.284084295351\n",
      "epoch = 325: E = 0.282075886171\n",
      "epoch = 326: E = 0.284171160353\n",
      "epoch = 327: E = 0.284084295351\n",
      "epoch = 328: E = 0.282075886171\n",
      "epoch = 329: E = 0.284171160353\n",
      "epoch = 330: E = 0.284084295351\n",
      "epoch = 331: E = 0.282075886171\n",
      "epoch = 332: E = 0.284171160353\n",
      "epoch = 333: E = 0.284084295351\n",
      "epoch = 334: E = 0.282075886171\n",
      "epoch = 335: E = 0.284171160353\n",
      "epoch = 336: E = 0.284084295351\n",
      "epoch = 337: E = 0.282075886171\n",
      "epoch = 338: E = 0.284171160353\n",
      "epoch = 339: E = 0.284084295351\n",
      "epoch = 340: E = 0.282075886171\n",
      "epoch = 341: E = 0.284171160353\n",
      "epoch = 342: E = 0.284084295351\n",
      "epoch = 343: E = 0.282075886171\n",
      "epoch = 344: E = 0.284171160353\n",
      "epoch = 345: E = 0.284084295351\n",
      "epoch = 346: E = 0.282075886171\n",
      "epoch = 347: E = 0.284171160353\n",
      "epoch = 348: E = 0.284084295351\n",
      "epoch = 349: E = 0.282075886171\n",
      "epoch = 350: E = 0.284171160353\n",
      "epoch = 351: E = 0.284084295351\n",
      "epoch = 352: E = 0.282075886171\n",
      "epoch = 353: E = 0.284171160353\n",
      "epoch = 354: E = 0.284084295351\n",
      "epoch = 355: E = 0.282075886171\n",
      "epoch = 356: E = 0.284171160353\n",
      "epoch = 357: E = 0.284084295351\n",
      "epoch = 358: E = 0.282075886171\n",
      "epoch = 359: E = 0.284171160353\n",
      "epoch = 360: E = 0.284084295351\n",
      "epoch = 361: E = 0.282075886171\n",
      "epoch = 362: E = 0.284171160353\n",
      "epoch = 363: E = 0.284084295351\n",
      "epoch = 364: E = 0.282075886171\n",
      "epoch = 365: E = 0.284171160353\n",
      "epoch = 366: E = 0.284084295351\n",
      "epoch = 367: E = 0.282075886171\n",
      "epoch = 368: E = 0.284171160353\n",
      "epoch = 369: E = 0.284084295351\n",
      "epoch = 370: E = 0.282075886171\n",
      "epoch = 371: E = 0.284171160353\n",
      "epoch = 372: E = 0.284084295351\n",
      "epoch = 373: E = 0.282075886171\n",
      "epoch = 374: E = 0.284171160353\n",
      "epoch = 375: E = 0.284084295351\n",
      "epoch = 376: E = 0.282075886171\n",
      "epoch = 377: E = 0.284171160353\n",
      "epoch = 378: E = 0.284084295351\n",
      "epoch = 379: E = 0.282075886171\n",
      "epoch = 380: E = 0.284171160353\n",
      "epoch = 381: E = 0.284084295351\n",
      "epoch = 382: E = 0.282075886171\n",
      "epoch = 383: E = 0.284171160353\n",
      "epoch = 384: E = 0.284084295351\n",
      "epoch = 385: E = 0.282075886171\n",
      "epoch = 386: E = 0.284171160353\n",
      "epoch = 387: E = 0.284084295351\n",
      "epoch = 388: E = 0.282075886171\n",
      "epoch = 389: E = 0.284171160353\n",
      "epoch = 390: E = 0.284084295351\n",
      "epoch = 391: E = 0.282075886171\n",
      "epoch = 392: E = 0.284171160353\n",
      "epoch = 393: E = 0.284084295351\n",
      "epoch = 394: E = 0.282075886171\n",
      "epoch = 395: E = 0.284171160353\n",
      "epoch = 396: E = 0.284084295351\n",
      "epoch = 397: E = 0.282075886171\n",
      "epoch = 398: E = 0.284171160353\n",
      "epoch = 399: E = 0.284084295351\n",
      "epoch = 400: E = 0.282075886171\n",
      "epoch = 401: E = 0.284171160353\n",
      "epoch = 402: E = 0.284084295351\n",
      "epoch = 403: E = 0.282075886171\n",
      "epoch = 404: E = 0.284171160353\n",
      "epoch = 405: E = 0.284084295351\n",
      "epoch = 406: E = 0.282075886171\n",
      "epoch = 407: E = 0.284171160353\n",
      "epoch = 408: E = 0.284084295351\n",
      "epoch = 409: E = 0.282075886171\n",
      "epoch = 410: E = 0.284171160353\n",
      "epoch = 411: E = 0.284084295351\n",
      "epoch = 412: E = 0.282075886171\n",
      "epoch = 413: E = 0.284171160353\n",
      "epoch = 414: E = 0.284084295351\n",
      "epoch = 415: E = 0.282075886171\n",
      "epoch = 416: E = 0.284171160353\n",
      "epoch = 417: E = 0.284084295351\n",
      "epoch = 418: E = 0.282075886171\n",
      "epoch = 419: E = 0.284171160353\n",
      "epoch = 420: E = 0.284084295351\n",
      "epoch = 421: E = 0.282075886171\n",
      "epoch = 422: E = 0.284171160353\n",
      "epoch = 423: E = 0.284084295351\n",
      "epoch = 424: E = 0.282075886171\n",
      "epoch = 425: E = 0.284171160353\n",
      "epoch = 426: E = 0.284084295351\n",
      "epoch = 427: E = 0.282075886171\n",
      "epoch = 428: E = 0.284171160353\n",
      "epoch = 429: E = 0.284084295351\n",
      "epoch = 430: E = 0.282075886171\n",
      "epoch = 431: E = 0.284171160353\n",
      "epoch = 432: E = 0.284084295351\n",
      "epoch = 433: E = 0.282075886171\n",
      "epoch = 434: E = 0.284171160353\n",
      "epoch = 435: E = 0.284084295351\n",
      "epoch = 436: E = 0.282075886171\n",
      "epoch = 437: E = 0.284171160353\n",
      "epoch = 438: E = 0.284084295351\n",
      "epoch = 439: E = 0.282075886171\n",
      "epoch = 440: E = 0.284171160353\n",
      "epoch = 441: E = 0.284084295351\n",
      "epoch = 442: E = 0.282075886171\n",
      "epoch = 443: E = 0.284171160353\n",
      "epoch = 444: E = 0.284084295351\n",
      "epoch = 445: E = 0.282075886171\n",
      "epoch = 446: E = 0.284171160353\n",
      "epoch = 447: E = 0.284084295351\n",
      "epoch = 448: E = 0.282075886171\n",
      "epoch = 449: E = 0.284171160353\n",
      "epoch = 450: E = 0.284084295351\n",
      "epoch = 451: E = 0.282075886171\n",
      "epoch = 452: E = 0.284171160353\n",
      "epoch = 453: E = 0.284084295351\n",
      "epoch = 454: E = 0.282075886171\n",
      "epoch = 455: E = 0.284171160353\n",
      "epoch = 456: E = 0.284084295351\n",
      "epoch = 457: E = 0.282075886171\n",
      "epoch = 458: E = 0.284171160353\n",
      "epoch = 459: E = 0.284084295351\n",
      "epoch = 460: E = 0.282075886171\n",
      "epoch = 461: E = 0.284171160353\n",
      "epoch = 462: E = 0.284084295351\n",
      "epoch = 463: E = 0.282075886171\n",
      "epoch = 464: E = 0.284171160353\n",
      "epoch = 465: E = 0.284084295351\n",
      "epoch = 466: E = 0.282075886171\n",
      "epoch = 467: E = 0.284171160353\n",
      "epoch = 468: E = 0.284084295351\n",
      "epoch = 469: E = 0.282075886171\n",
      "epoch = 470: E = 0.284171160353\n",
      "epoch = 471: E = 0.284084295351\n",
      "epoch = 472: E = 0.282075886171\n",
      "epoch = 473: E = 0.284171160353\n",
      "epoch = 474: E = 0.284084295351\n",
      "epoch = 475: E = 0.282075886171\n",
      "epoch = 476: E = 0.284171160353\n",
      "epoch = 477: E = 0.284084295351\n",
      "epoch = 478: E = 0.282075886171\n",
      "epoch = 479: E = 0.284171160353\n",
      "epoch = 480: E = 0.284084295351\n",
      "epoch = 481: E = 0.282075886171\n",
      "epoch = 482: E = 0.284171160353\n",
      "epoch = 483: E = 0.284084295351\n",
      "epoch = 484: E = 0.282075886171\n",
      "epoch = 485: E = 0.284171160353\n",
      "epoch = 486: E = 0.284084295351\n",
      "epoch = 487: E = 0.282075886171\n",
      "epoch = 488: E = 0.284171160353\n",
      "epoch = 489: E = 0.284084295351\n",
      "epoch = 490: E = 0.282075886171\n",
      "epoch = 491: E = 0.284171160353\n",
      "epoch = 492: E = 0.284084295351\n",
      "epoch = 493: E = 0.282075886171\n",
      "epoch = 494: E = 0.284171160353\n",
      "epoch = 495: E = 0.284084295351\n",
      "epoch = 496: E = 0.282075886171\n",
      "epoch = 497: E = 0.284171160353\n",
      "epoch = 498: E = 0.284084295351\n",
      "epoch = 499: E = 0.282075886171\n"
     ]
    }
   ],
   "source": [
    "y_sam, E=sammon(X, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnX90HNWV57+3LdlSsGwUDAZibGGZBYOxHCMIJvyaXbAZHw67E4ccmMw4CdbBZLIL2SSzE4adyZnMyWwMJNEAmcRIZmJCxsyeJRkGMXjAIGyzawdkJKPwK/wS+Ac2BsuSMLJkq+/+USqr1Krurur6Xf39nFNHqu7qV7eqX3/fe/fdd0tUFYQQQtJDJmoDCCGE+AuFnRBCUgaFnRBCUgaFnRBCUgaFnRBCUgaFnRBCUgaFnZCYIiKfF5E3RORjEfkvUdtDkgOFnUSGiFwqIv9PRPpE5KCI/F8RuTBqu6JARJ4Vkaacl78P4D5Vnaqq/xKFXSSZVERtAClPRGQagDYAXwfwvwFMBnAZgKEo7YoZcwC8HLURJHmwx06i4j8AgKpuUNURVR1U1SdV9SUAEJF6EXlGRD4SkQ9F5FcicqL5YRHpEZE/F5GXROSwiKwTkZki8oSIDIjIJhGpHT22TkRURL4mIrtEpFdEbhGRC0c/f0hE7rOUnRGR/yki74rIByLyoIhMzynrKyLy3qhtd9hdoIicOVp2ZnS/VUQ+sLz/kIh8U0R+AKNRu2/U7XKfiLwFYC6Ax0Zfm+L3F0BSjKpy4xb6BmAagI8ArAfwhwBqc96fB+BqAFMAnAxgC4Bmy/s9ALYDmAngMwA+APAigM+OfuYZAN8bPbYOgAL4OYAqAEsBHAHwLwBOsXz+itHjbwLwJgxhnQrg1wB+mVNWC4BqAA0wRhnz81znewAuGP3/dQBvm8eOvvfZ0f+fBdCU89keAFdF/V1xS97GHjuJBFXtB3ApxkTygIj8q4jMHH3/TVV9SlWHVPUAgB8DuCKnmHtVdb+q7gGwFcBvVbVTVYcA/AaGyFv5W1U9oqpPAjgMYIOqfmD5vHn8lwH8WFXfVtWPAdwO4AYRsbou/0aNUcZOADthCLwdmwFcISKnju7/n9H9M2E0bjsd3jJCHENhJ5Ghqq+q6ldVdRaABQBOB9AMACJyiog8LCJ7RKQfwEMAZuQUsd/y/6DN/tQSjz8dwLuW996FMR810/LaPsv/n9icy2QzgCsBXA5j1PEsjAbqCgBbVTWb53OElAyFncQCVX0NwC9gCDwA/C8YvfmFqjoNwJ8AkJDM2Qtj4tJkNoBjGN8QOGUzDP/5laP/Pwfg8zCEfbPlOKZZJb5BYSeRICLniMi3RWTW6P4ZAG6E4TcHgBoAHwM4JCKfAfDnIZq3AcB/H538nArg7wD8s6oec1uQqr4BYzTwJwC2jLqg9gNYgfHCvh+GT58Qz1DYSVQMAPgcgN+KyGEYgv47AN8eff9vACwG0AfgcRgTmGHxAIBfwnCdvANjovW/eShvM4CPVPU9y74A6LQc8/cAvjgasXOPh3MRAlHlCJAQQtIEe+yEEJIyKOyEEJIyKOyEEJIyKOyEEJIyIkkCNmPGDK2rq4vi1IQQklh27NjxoaqeXOy4SIS9rq4OHR0dUZyaEEISi4i8W/woumIIISR1UNgJISRlUNgJISRlxOYJSkePHsXu3btx5MiRqE3JS1VVFWbNmoXKysqoTSGEkLzERth3796Nmpoa1NXVQSSsJH7OUVV89NFH2L17N84888yozSGEkLzExhVz5MgRnHTSSbEUdQAQEZx00kmxHlEQQggQI2EHEFtRN4m7fYQQAsTIFUOSjWoW/f3Pw5qyXKQC06ZdhNFnORNCQoLCnsPGjRtx2223YWRkBE1NTfjud78btUmJYGhoFzo7l2DSpBoYA8EsRkYGcPHFPaiqmlPs44QQH2FXysLIyAi+8Y1v4IknnsArr7yCDRs24JVXXonarERQVTUHtbVLMTJyGCMjfRgZOYza2mUUdUIiINnCfuAA8MILxl8feP755zFv3jzMnTsXkydPxg033IBHH33Ul7LLgblz1yCTmQIAyGSmoL5+TcQWEVKeJFfYN2wA5swBrr7a+Lthg+ci9+zZgzPOOOP4/qxZs7Bnzx7P5ZYLNTWLMH36ZQAE06dfjqlTG6I2iZCyJJnCfuAAsGoVMDgI9PUZf1et8txzt3tMICNh3FFffycAYW+dkAhJprD39ACTJ49/rbLSeN0Ds2bNwq5du47v7969G6effrqnMsuNqVMbcMkle9lbJyRCkinsdXXA8PD4144eNV73wIUXXog33ngD77zzDoaHh/Hwww/juuuu81RmOTJ58syoTSCkrEmmsJ98MrBuHVBdDUybZvxdt8543QMVFRW47777sGzZMsyfPx9f+tKXcN555/lkNCGEhENy49hvvBG46irD/VJX51nUTZYvX47ly5f7UhYhhERBcoUdMMTcJ0EnhJC0kGxhJyQgkpIiobcXqK2N2goSNzwLu4icAeBBAKcCyAK4X1X/3mu5hERJElIktLYCq1cDa9cCTU1RW0PihB9dj2MAvq2q8wFcDOAbInKuD+USEhlxT5HQ2grceiuQzRp/W1ujtojECc/Crqrvq+qLo/8PAHgVwGe8lktI1MQ1RYIp6oODxv7gIMWdjMdXZ6GI1AH4LIDf2rx3s4h0iEjHAZ9yuxASJHFMkdDba7hfTFE3GRw0Xu/tjcYuNyTBxqTjm7CLyFQAjwD4pqr2576vqveraqOqNp4c00iWm266CaeccgoWLFgQtSkkJsQtRUJtreFTr64e/3p1tfF63CdSW1uBGTM4uggaX4RdRCphiPqvVPXXfpTpFD9b/69+9avYuHGjfwXGCNUs+vq249Ch545vfX3boZqN2rRYE8cUCU1NwD33jIl7dbWxH/cJVM4LhIcfUTECYB2AV1X1x95Nco7fUQGXX345ejzmm4krSYjyiCtxTJFg1vfVq5Ml6rnzAkD8bU8ifvTYPw/gTwH8RxHpGt0CX7rJ1t8dcY/yIO5pagI+/DD+wpiGeYGk4UdUzHOqKqq6UFUXjW7/5odx+WBUQGnENcqDlE7cfepA8ucFkkh8ltA5hK1/6cQxyoOUB0mdF0gqiRN2tv7eiFuUB8DGuFwwxT2ToagHTeKEHQiu9b/xxhuxZMkSvP7665g1axbWrVvn3diYEbcoj7iGv7GxCYakzAsERVj1KpHCDgTT+m/YsAHvv/8+jh49it27d2PVqlXeC40hcYnyiOsEeFwbm7RQrqPqMOtVorM7NjUBK1aUb0VJMsXC36LKrpjb2Jj2EOKFsOtVooUdoKgnEXMCPJuzNsqcAF+xAqiuzh93P2XKGYGIPmOtSRBEUa8S64ohycXJBHihuHtzsVV393J0d1+L7u7l6OxcgqGhXfYndACjrUgQRFWvKOwJJA0i42QCPF/cfRCLrRht5R9pqJ9+EVW9orAnjDRN7BWbAC8Ud+9lsVU+4XETbcXcO/akqX76RSQx/Koa+nbBBRdoLq+88sqE1+JIlHa2tKhWV6sCxt+WlshM8ZWDB/O/NzDQpe3tGR0Y6JrwXlfXUm1vF+3qWub4XC0tqplM4Xvn5JjBwR5tb4du2VKjW7ZM1y1barS9HTo42OPYlrThpn4W+s7TipN6VQwAHepAYynsFt577z298sor9ZxzztFzzz1Xm5ubJxwTlZ3WH425pUncCzE0tM/29UKib8UUEb+Fx2hYMtreDm1vz7hqYNKGm/rph8AlFa8NGoW9BPbu3as7duxQVdX+/n4966yz9OWXXx53TBR2Hjxo/BCsPxpzy2TKs/djkk/0TUwRWbnS/4axv79TN2+u1vZ26ObN1UUbmLTipn6mddQZFk6FPZE+9qD8m6eddhoWL14MAKipqcH8+fOxZ88eP0z2BCf28mNdbJVbLx566Dn89KdGvXjwQf8jE5h7x8Bp/WTyvvBIZBx7GLnFe3p60NnZic997nO+lOcVc6LF/GEwidJErPVieDiDT386i5/8ZAA33NCD/fsn1gvzHtbWGuJeSgNZX38nOjoW+5p7p1RboqRY/XSydiFp1xxnEtljDzq3+Mcff4wVK1agubkZ06ZN86VMP2ASpcJY68WkSX2YMuUwnn9+WUFRb2oyeownnVRaz9Hv3DtJjiopVD856gwZJ/4avzc/fOxB+TeHh4d16dKl+qMf/cj2/ah87IX2yRjWerFxY7XW13eN86mvXDl+4q6lRbWiwni/oiJan68X/3Oc6kQhW+hj9wbS7GMHgvFvqipWrVqF+fPn41vf+pZ3I33ArgfH3k1+rPXi2LHLsXevUS/MHvr69WPZBVtbga9/HTg2mpng2DFjP4reshf/c9x6+YXqJ0edIeFE/f3e/IqKcRru5pStW7cqAD3//PO1oaFBGxoa9PHHH/dsZ6mwd1Ma1nqRL7Tu4EH7KA5zC7MH7CXqKal1JE4jjCQBhz32RE6empj+Tb/S0F566aUw7p0zgsxAyIRUpWOtF4UygIoY8mn3ulfc1A3T/2z9voHxk7t2JLmOcNQZLOJGyPIWIvIAgGsBfKCqC4od39jYqB0dHeNee/XVVzF//nzPtgSN1c4jR97F9u11vkfn9PYaQ+vcCALAGMJ++GE8fxhxi+YoZk9rK/BnfwYcPTr2WmUl8A//4F4Yc4V8eHgfXnnlemQyUyEyCU7qhlWoi0U9JbWOEG+IyA5VbSx2nF8+9l8AuManshJDUNE5SYwgCNrP6zbW3Ik9TU2GiFdWGvulijowFmr50ktGxsnXX78JAJDNOq8bbvzPSawjJDx8EXZV3QLgoA/l+GBNcNjZ5yUZVSGS9PDfoJ+E5LbRcGOPKe6ZTOmiDhiN/OHDS3H06JiQT5u2BJlMFQDndcPto+MGB8capjjXERIyThzxTjYAdQB+V+D9mwF0AOiYPXv2hEmBt99+Ww8cOKDZbNbHqQb/yGazeuDAAX377bcnvFdKMiqnxD2vRtA5bNxODpZqj9fJvJYW1fPO69QnnjBCLZ9+2gjBDapuWK+zokJVJL51hPgHHE6e+uJjBwARqQPQpiX62M3njB45csQXe4KgqqoKs2bNQqXZRRrl4493oqNjMRobXwxkWXncfNcmQft5cycHgcK90qj8zlY716xZhsbGp/Dii0tx4okbccMN/tcNu/tSVQXcey9762nHqY89tB67dbMLd0w6xZJRpZWgeuylhgCGnQUz1876+i59+umM1td3HbfTz7rBhHDlDdK+QClu+BVymTSCmgsoNjl44on2ieBWrcqGOjeRa+dbbzXgi1/ci717G45PYvpZNzhpShzhRP2LbQA2AHgfwFEAuwGsKnR8Gnvs5Y51LsDPXmM+H3uxB12EPTcR9kKhpC5MIt5A2D52N9j52Eny6e0FHnnEyNa3dq1/veTWVvsyd+5cht7eTQCyADKorb0aDQ0bx9kTZg82n51eMOPj+/uPoabGeM1c6LRuXcb380VBkAv90oZTHzuFneTF7Q/OzQIbt9iJ9MBAFzo7L0E2O4hMphqLF28LJSd6ofty6FDGtjEptZExF8EdPlyDqqoMJk8ev9CplHLjJqRBLfRLI06FPdEpBdJKXKJg3OS9D3p5u939MBN+9fY+FeqDLgrdl9raiULkpSf/0ENz0Ne3FIsWbcKkSVkcO5bBpz89ttAp9744qTthPM/ADeZCv/GjL//ScJcjHOfEjDhl6nO6stZ8iEKhJxSV+pSiYpx88p0ApOjiHydP3XJqo5sVx14Wb5mf/dnP1uDoUWMR3PDwFOzcaX+tTuuOnf1Tp3oTUrffb+7xQS30K1co7AFSyjL4IFdwloKTH1yxSI1HHnHeWLm5Z62twBlnNOCdd4o/6MLspXZ3L8dLL12L7u7l6OxcgqGhXcfLsrMxnz1O7kuxVLyFrtXaWL711iK89NJlyGYFO3dejq99rWHCZ93WHav92ewU3HjjmpLrWykrg3OP52MGfcbJDKvfWzlExbiNygg7/toNTldP2kVquInecHPPSokK6epaqs88k9H2dugzz2SOX0++sorZU+i+FIs3b24ufq1Wu8z4+HPP7ZrwmVLrjnE/RO+6a1nJ9c3LyuDc4/1Ow51G4DAqhsIeAG4re9wXnbj5wVnF0I3gtLSofupTIzp//ja94IKt+stfbtXe3q166NA2zWZHJhybr9yPPhrRQ4e2aW/v1uObWcb69Z26cePY05XWr++aUJbIiC5atE2/852tesEFW3XBgq26aNE2bWkZmWBzsfuSz86VK1UrK439ykrn4n7aafts88qXWnfWrx9bTFVKZ8Jtg+Lk+HJd6OcUCntElNp7imuP3RQGNz+4gwfdCY557TNnGrHpbW01+thj03XTpvGx6WbZ+coVUT31VPv49n/8xx6trlZds2apPv206Jo1y46Lq3XLtaGtzfj8nDk9tt9FsfuS28hbRd3ccsXdvDfm32Ijh1Lqjnkfa2v3ldSZcNugxL3zkhQo7BHgtfLGbdGJ10U+TgQn956tWbNUN20y3CWbNmX0hRcmujnsyq2sHHt26d13j7lc2tszum3bsuPnsC75Nz9bVTW+rFwb1qxZdvw7LDQiKHYfm5uNxidfo3Tw4Nixuc9mDaLueO1MBNFjJ4WhsEeEHz+WOGRz9KuRcVLOeF/yxAyJxcq1irpZhulyefrpap03r2tcT9naS62oMETUauM3vzn2+SeeMB6IbdpebMVrPsxRjJ2om1tzc/6646RHW0rd8fo9++ljJ8WhsEeI18prioBXSi3D756VE8GxntPocTubrM03QjJdLubEYG6vPHc0lTuZ2dY25rLJvXZj0nRsROAmJW9Ly/hGyK5xyd3MBsnJ/Xfynece48fIzG2gQBw6L0mEwh4xXiqvHxW/1DKC8oW66XGuX+98stZ0X+SK4tlnT3S5VFRMvDaraFttHBjo0meeyei8eROjUPr7O3XzZqNHv3mzMapwc1+s4l5RMdaoFOrNm42TVzHMVy+8diTcfj5OPvU42VIMCnsMKKXC+DFU9XN47bTH7tePo5TJWtWJ15zrcskVSCdCOTS0L+91WUMdS3WBWB+OYXfP7bag3WLlRtJGDxT2BOKHC8QvN4obEYjLj8NpqGVLix6fsCzVZjPUcf36rrz3qVBjlyvs5mtOxL2UkVO5TVxms8UnuZPY0FHYE4YfLhC/3ShufeNx+HHYhVHm2pb7enNzaed64IF9BRuP3HtnDV/Md89aWkb0vPOMOPrzz9+qDQ1bdf78bSoyUvQe5/t+4xRq6ERw/cBJWuckNnSpFfYk+cPcEqceu0mxXmfcfxy5ApuvV7xypbtyi8XTm+6eXKG3myS13jOrIG3ePBbLP3NmT9FQQr9j3YOg1KiiUsg3yR2nhs4tqRT2uAz5gyQOPnYnJOnHYV3wU2iS0m3P3U4sc0MvAWPfbjFUvnuWK0htbcsci7bTOPIoG2AvUUVusJvkNolLQ+eW1Al7XCplGPgRfubVh+z0PEn7cRTyY/sxFyFipEVYsGDr8c3qSnFyXjdRN26/gzh0jtavH1urkCu4flMon08SNSVVwp5EAfGClzh2673yIzzOzfmS8p00NzvrOTvFKpZf/7p9SoKZM3tsz1dRYX/PiiVey2ZHdNeubbpwoX0DUug6ohxNmfXFXGfQ1hZMb93EST6fqBs6N6RG2JM05PcDLxUtqgYwaT8OVcPX7UeP3cSaHydfSgK7LV8SsGKCZPqqN22a2IAENafiFWv9LJSt0m+Khc4mSUNCFXYA1wB4HcCbAL5b7Hj22O3x0vuNugFM0o/DxLqE36/61NKiet55Y66GJ56o1rPP7tJJk9yPEooJktVXbTYgQa8QLhW7+mmuM0hjBy0oQhN2AJMAvAVgLoDJAHYCOLfQZ+hjn0gcI2LiQNA/+CDErKXFSItgpjRoaRlz/+ROonr5fqy+eDMnTqGynEQ4mWmLzbTJfoYjprF+hk2Ywr4EwL9b9m8HcHuhz0QVFWONoX3vveBiaN3iZ287TQ1gWC6eIBoPM9e5mfPdzEdj9/3Ynd+pTaYv/oUXlhUV7nz30mqTmbb48cdrdNOmsXDEfft6nBlUhDTVzygIU9i/CKDVsv+nAO6zOe5mAB0AOmbPnl3yhXn5Eeb6Je3yfUeFn72ZJPq8c0mDAOzfv8/2Oqzfj9135eb7K5TTxlpeodWxuZ0K6xyBk3BLt6ShfkZFmMJ+vY2w31voM1GuPG1rGz+xFfSsvBv8FLMk+yzDGrIHvQqy0HXkJi+zir6bOtDSYjxZKd/xTu5l7jG5aY/PPbfL9+8gyfUzSlLpivGK3cRWGLPybkh7b8bvJ/N4IchVkE6eeepkcZPTBUd2x1ttyI2vX7hwq+7aNdaI5TYobW3jn4dKn3g8CFPYKwC8DeBMy+TpeYU+E4WwWyu59fFocZyVj5MtfuK00Qpzks1uFaRf9z/fdThN01uoQXPaAJphnU4eO2j9fvbsmZj2OMwoK2JP2OGOywH8fjQ65o5ix0fZY6+uHv94NPZAwqEUF0Opbik3LpbcVZ7r13f57k8ulogsqB577vu5vnO7xU9WwS6U5CxMwkoclgRSs0DJb8zKXlu7j6IeEqX2wEt1S7l1sZiRJW1tywKZsM13HX752J02HPX1nfrUU+6W8sdhEjvMxGFxh8JegLT7seOEV595qUN+N4mmzMgSc5KwlN5psV5loVwvXqJi7I4vdM/vvLNwqgKnNoZNoUyN5YRTYRfj2HBpbGzUjo6O0M9rpbcXqK2N1ISyobUVuPVWYHBw7LXqauCee4CmJvvPqGbR3/88VI8df02kAtOmXQSRTNFzDgx0obPzEmSzg8hkqrF48TZMndpge2xvL3DWWfvx0UczJ7yXyQAffli8rhw58i62b6/DpEk1ADIAshgZGcDFF/egqmpOwc9a66J53f39x1BTY7zm5Lpz63O+e/7zn+/E7NmL0dj4Yt774aT8sLH7Ph9+uAGrVwNr1+avR2lDRHaoamOx4yrCMCaOUNTDw/zRmUJTTNQBYGhoFzo7l5QklABQU7MI06dfht7epzB9+uUFRay2FvjhD2fmbXyc1JWqqjmorV2K3t5NALIAMqitXebIVmv5pV53ro357vnKlQ0YHt6LyZMnNmJObYyC3O/z4YcbcOutQDZrXCNQPuLuhOJdH0J8oKnJEJZMprioA2NCOTJyGCMjfRgZOexYKE3q6+8EIKivX+PYvupqY99J42OltxeYO3cNMpkpAIBMZoqj8+bix3Wb5LvnbkU9Lpjf586da8Y1woODhri3tkZqXqwoW1cMiQY3Q3o37pR8DA/vdyVkra1wPby3fubCC5eht/cp1NYuRUPDRle2mvhx3VaidqP4yQcf7Mdpp81ENjvxPadusyTj1BXDHntI9PZGbUE8cPOjM4ffgBR1p+TDbe+0qckQBzeibnUJ7NzpfJSQDz+u20qahO6UU2Zi7dqxkZVJdbXRsKbpWr1AYQ+B1lZgxgwOFUvBjTvFL5yKQ2srcNttWdTVbceCBc+hvv453HPPAPbseQwnnHC+JxuiuO6k4NVtVg7QFRMw1uiENFXAMIf3bt0pYdDbazTWJ5/8Lh5+uA6HD9dANQORLE44YQDz5/dg5kz3fnErcbzuOFGK2yzp0BUTA3JDztIyyRP2CKRUcfPi/lLNoq9vOw4deu741te3HaqGc7e21hCU/v45eP75paiqOoypU/tQVXUYhw8v8yzqQHInOcPCrdusnGCPPSDMHl3aJnmKjUC8xp/7aaeX3pzTuPTWVqC5uQt3330JqqoGkc1W46KLvE12AvG5j25I0yRtXGEce8SYPTovsdFxI98IBBgTT6/x537a6SXG2WlculHuInR3X4bGxqdw0kneJzuBeNxHN5SjWyTOxLPpTwlpmuTp7TV+uNZGCjD2V68ec3sUi8MOOjrIT/eX07j0pibgy1++EyL+TXb6Gc8eNLkNadJdjWmAwh4wbhfmxBVzBOIkzCyfIAbtm3fa+DjFTdjh6ac34JJL9vrSWzfxY8FT0KR1HinxOEko4/cWdRKwKEhLsiKn2f7MjIlmsqawsgT6nct9YKBL29szjjIhFsJbMjN3SbvCIsyHohADMLsjCQon2f6sghj20+n9bkSGhvZ5tqfU7Ih+NSxBEfZ3W+44FXZGxZCScBIBMTy8H4cPz4wkOqi1Fbj5ZkNqWloKu8CCjEDxYx1D3OPZ07pWI44wKoYESjExVs1icPAdiLyB9euB5mbgk08q8NprF0E1U3J0kBsRnjIFOHKkeGSMnxEo1gbPSRSRE3JF/eDBLCZNik8opHktq1dT1OMCe+wkEHLjwIeHs5g0aQA33NCD/v45JQuAk/jyUvK/79y5LCe08WrXSbysIX8rVgSzjqG1Ffirv3oXGzaUlvs9SBjHHjyhrDwVketF5GURyYpI0ZOR8iE3XG/SJGNF5oEDpYu6Xbl24ZSlRMZ4jUDJDfl75BHnUURuz7Fv3xzs2LEUx47FKxSSoh4fvI7bfgfgCwC2+GALSRm5YnnFFWt8WQJeSITdhGVa8ZJRsZDLxa91DLnn+NnP1mB4ON6hkCQ6PAm7qr6qqq/7ZQxJF3Zi6UevrpgIl7owrJSMioVGCLfcksU112zH/fc/h4ULn8P99z+H668fyzfj5RxvvbUIO3dehmxW8KlP+bPaNQqYzjoYQptpEZGbRaRDRDoOHDgQ1mldwUrmP0Glny1WbikLw6ZOdb/IqNAIobV1F958cwnmzFmOe++9FnPmLEdn5xIMDe1yXH6hc6xfb6x2PeecZPbW3SxYK5aUjYyn6OSpiGwCcKrNW3eo6qOjxzwL4Duq6mhGNI6Tp8x1ERxBhes5KTesCb18IX9uJ2ULRf2sW5exeYZpvEMh8+E2RNLLw8LThG/hjqp6lT8mxRc/kkaR/JQiPE4E2Um5+crwO3Y9X8jf3LlrLI+5K+4LLxR62dQ0x+YcyRZ1wFkYqJeHhZcjZZ8rJk25LuLmSirVnjDyvZsC2t29HN3d16K7uzQ3iRW7/OBuJ2WrquZg6tT8UT9Jz0HuJZ9PEnLnxAWv4Y5/JCK7ASwB8LiI/Ls/ZoUlU+BxAAAMIElEQVSD30mjoiRuj98r1Z6wMgUGlT3RboTgZp6htRX44z9eg2w2v4AlOayw1KglwP9nwaYaJ3kH/N7ilCsmDbkuwkqw5Zc9+ZJDhf1d9Pd36ubN1dreDt28uTrQfCxO8s1Yr//uu5fqM8/EM/mXH5RaZ+OeOydowCRgzombMLohbg1TMXvyJcSKKlNgXLIn5t63+vouffrpjK5fn14BKzU5mtekbEmGwu4SLxn4wiJX3OKWNrWYPc3NhRvQKBqpOPQADx5UFZl4z2pr96U+/W2ary0IKOwlEOdKlq/hSUqPfeVKZ3ZGMXqKogdorWstLaoVFROF3en1x7neEn+hsKeIYmIXN1dSrj3Nze5GFmGOng4eDF8Yrddn1xACqpWVzq4/CSNN4h8U9pTgtEcetx94rj1uRxZhiG1Li2GHSHj3LbfRs2vszK3YPYiqQecIIToo7CnArQ89bj84u554XEYWue4Ppz1kt+S6XHLFvLJStbJyROfP36YLFmzVBQu26gUXbNUHH9ym2exIQfujcMHFrQNRblDYU0LcfOheiYMw5PNp+y3u1mst1EifemqPtrdD29pq9LHHpuumTTXa3g4dHOyxLTeqSfM4NczlCoU9RaTtBxXlyCJfBIrfwmj3nRVqpNvaluqmTRltb4e2t2eKhl9G+RzZNHQwkgqFPWXY9XTj5npJCkH32AuJYL5Gur+/U5991t1iqbAa/LiF1ZYzFPYUkuuvjdqlkWSC8rE7EcF8310pi6XCqgfssccDCnuKSZtrJiqCiopxIoJ2vdxSF0uF1WNmvYsep8LOh1knjFIe1EzyYyZ68zuxltt84yZOc9f7nXbYKXxuQbQ4zcdOYU8Qvb1GxsSszUNjMhkjnWuSM/+ljSBFMMoHT4T18BIyEafCXvb52JOEl5SnxJ4gUzMHmTv9oYfm4IUXluLYMX/TDudid39Yz+IPhT1hlPqgZjKRfDnj/RT7IETQdPOsXbsGw8PBPXgibjn+iXMo7AmklAc1OyVJDxfxQr4HesRdzKy++7feWoSdOy9DNisYGPD3wRNhPfCEBISTGVa/N0bF+IPf0RDlEkLpJANlHKM+7EIpzbzt8+Z1+VYfGNoYX8BwR+KGcgllKxRnnrvF8T7Yie5pp+3zzU4uRoo3ToWdrhiSqgd6FyPfBLQdcXz2rd0cy/e/P9M3dxwn6NOB14dZ3yUir4nISyLyGxE50S/DSDik6YHeTrETx5Urgaqq8cfFVcyCnGOxls8J+uTitcf+FIAFqroQwO8B3O7dJBImaemhuW2AcsXxssuAoSGgosJ4342YqWbR17cdhw49d3zr69sOVZsFBz4RZCilWX6QjQcJFk/CrqpP6tjSt+0AZnk3iYRN0ntoTiJZ7ITfFEfAcD2pAiLGvpvrHxrahc7OJejuXo7u7mvR3b0cnZ1LMDS0y92FuCToRjfoxoMEh58+9psAPJHvTRG5WUQ6RKTjwIEDPp6W+EFSe2hOwvIKCf8jj4yfXzh61Jn/3UpV1RzU1i7FyEiwi4WiICkjNjKeoikFRGQTgFNt3rpDVR8dPeYOAI0AvqDFCgRTCsSZJC0Xd5I3p1DOFj9TNAwMdKGz8xJks4PIZKqxePE2X+PKCQF8TCmgqlep6gKbzRT1rwC4FsCXnYg6iTdJEXUnk77Fon38nF+oqVmE6dMvAyCYPt3fxUKEuMVrVMw1AP4CwHWq+ok/JhFSnGKiDDiL9vFzfqG+/k4A4vvSfkLc4tXHfh+AGgBPiUiXiPzcB5sIcUQhUXbTG/drfmHq1AZccsle9tZJ5FR4+bCqzvPLEEJKwRTh1asnirL5v5O86E1NwIoV3l1RTnKpExI0noSdkDhQSJQLCX8uSZlfIKQYFHaSCgqJsl+9cUKSAnPFkLKAok7KCfbYCQmZqJ5XSsoHCjshIWOmIIjieaWkPGD3gJCQSXMKAhIPKOyERMDcuWuQyQT3vFJS3lDYCYkApiAgQUJhJyQimIKABAUnTwmJCDMFAVerEr9hj52QCKGokyCgsBNCSMqgsBNCSMqgsBNCSMqgsBNCSMqgsBNCSMqgsBNCSMqgsBNCSMqgsBNCSMrwJOwi8rci8tLog6yfFJHT/TKMEEJIaXjtsd+lqgtVdRGANgB/7YNNhBBCPOBJ2FW137J7AgD1Zg4hhBCveE4CJiI/ALASQB+APyhw3M0AbgaA2bNnez0tIYSQPIhq4U62iGwCcKrNW3eo6qOW424HUKWq3yt20sbGRu3o6HBrKyGElDUiskNVG4sdV7THrqpXOTznPwF4HEBRYSeEEBIcXqNizrLsXgfgNW/mEEII8YpXH/sPReRsAFkA7wK4xbtJhBBCvOBJ2FV1hV+GEEII8QeuPCWEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJThi7CLyHdEREVkhh/lEUIIKR3Pwi4iZwC4GsB73s0hhBDiFT967D8B8D8AqA9lEUII8YgnYReR6wDsUdWdPtlDCCHEIxXFDhCRTQBOtXnrDgB/CWCpkxOJyM0AbgaA2bNnuzCREEKIG0S1NA+KiJwP4GkAn4y+NAvAXgAXqeq+Qp9tbGzUjo6Oks5LCCHliojsUNXGYscV7bHnQ1W7AZxiOWEPgEZV/bDUMgkhhHiHceyEEJIySu6x56KqdX6VRQghpHTYYyeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSeEkJRBYSckRvT2Rm0BSQMUdkJiQmsrMGOG8ZcQL1DYCYkBra3ArbcC2azxl+JOvEBhJyRiTFEfHDT2Bwcp7sQbFHZCIqS3F1i9ekzUTQYHjdfpcyelQGEnJEJqa4G1a4Hq6vGvV1cbr9fWRmMXSTYUdkIipqkJuOeeMXGvrjb2m5qitYskF9/ysRNCSscU8dWrKerEOxR2QmJCUxOwYgXdL8Q7dMUQEiMo6sQPKOyEEJIyKOyEEJIyKOyEEJIyKOyEEJIyRFXDP6nIAIDXQz9xuMwA8GHURoQArzM9lMM1Asm+zjmqenKxg6IKd3xdVRsjOncoiEhH2q8R4HWmiXK4RqA8rpOuGEIISRkUdkIISRlRCfv9EZ03TMrhGgFeZ5ooh2sEyuA6I5k8JYQQEhx0xRBCSMqgsBNCSMqITNhF5G9F5CUR6RKRJ0Xk9KhsCQoRuUtEXhu9zt+IyIlR2xQEInK9iLwsIlkRSVUYmYhcIyKvi8ibIvLdqO0JAhF5QEQ+EJHfRW1LkIjIGSLSLiKvjtbX26K2KSii7LHfpaoLVXURgDYAfx2hLUHxFIAFqroQwO8B3B6xPUHxOwBfALAlakP8REQmAfgpgD8EcC6AG0Xk3GitCoRfALgmaiNC4BiAb6vqfAAXA/hGSr/P6IRdVfstuycASN0srqo+qarHRne3A5gVpT1BoaqvqmoaVxJfBOBNVX1bVYcBPAzgP0dsk++o6hYAB6O2I2hU9X1VfXH0/wEArwL4TLRWBUOkD9oQkR8AWAmgD8AfRGlLCNwE4J+jNoK44jMAdln2dwP4XES2EB8RkToAnwXw22gtCYZAhV1ENgE41eatO1T1UVW9A8AdInI7gP8K4HtB2hMExa5x9Jg7YAwDfxWmbX7i5DpTiNi8lrqRZbkhIlMBPALgmzmeg9QQqLCr6lUOD/0nAI8jgcJe7BpF5CsArgXwnzTBiwZcfJdpYjeAMyz7swDsjcgW4gMiUglD1H+lqr+O2p6giDIq5izL7nUAXovKlqAQkWsA/AWA61T1k6jtIa55AcBZInKmiEwGcAOAf43YJlIiIiIA1gF4VVV/HLU9QRLZylMReQTA2QCyAN4FcIuq7onEmIAQkTcBTAHw0ehL21X1lghNCgQR+SMA9wI4GcAhAF2quixaq/xBRJYDaAYwCcADqvqDiE3yHRHZAOBKGOls9wP4nqqui9SoABCRSwFsBdANQ3cA4C9V9d+isyoYmFKAEEJSBleeEkJIyqCwE0JIyqCwE0JIyqCwE0JIyqCwE0JIyqCwE0JIyqCwE0JIyvj/LyE2ApbTjTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b28a6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_sam[y==0, 0], y_sam[y ==0, 1], s=20, c='r', marker='o',label=0)\n",
    "plt.scatter(y_sam[y ==1, 0], y_sam[y ==1, 1], s=20, c='b', marker='D',label=1)\n",
    "plt.scatter(y_sam[y ==2, 0], y_sam[y ==2, 1], s=20, c='y', marker='v',label=2)\n",
    "plt.title('Sammon wtf')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45639169,  0.07408636],\n",
       "       [ 1.92768422,  0.25182436],\n",
       "       [-0.71361493,  0.27284756],\n",
       "       ..., \n",
       "       [-0.68951068, -0.47527581],\n",
       "       [-0.23592119, -0.20685205],\n",
       "       [-0.31193699,  2.23774418]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sammon_df=pd.DataFrame(y_sam)\n",
    "sammon_df.to_csv('sammon_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=0.28207588617081786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Neural networks are a good example of where going from the model form to\n",
    "the fitted model is difficult. We must use numerical methods, and convergence to a global optimum\n",
    "is not guaranteed. This means that to find the â€œbestâ€ fitted model (i.e., the model with the highest\n",
    "criterion value) we need multiple random starts.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>I'm just going to give up on sammon/ SOMs for text classification for now...</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_raw=pd.read_json('reviews_Video_Games_5.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asin', 'helpful', 'overall', 'reviewText', 'reviewTime', 'reviewerID',\n",
       "       'reviewerName', 'summary', 'unixReviewTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[8, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "      <td>07 9, 2012</td>\n",
       "      <td>A2HD75EMZR8QLN</td>\n",
       "      <td>123</td>\n",
       "      <td>Pay to unlock content? I don't think so.</td>\n",
       "      <td>1341792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "      <td>06 30, 2013</td>\n",
       "      <td>A3UR8NLLY1ZHCX</td>\n",
       "      <td>Alejandro Henao \"Electronic Junky\"</td>\n",
       "      <td>Good rally game</td>\n",
       "      <td>1372550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>1st shipment received a book instead of the ga...</td>\n",
       "      <td>06 28, 2014</td>\n",
       "      <td>A1INA0F5CWW3J4</td>\n",
       "      <td>Amazon Shopper \"Mr.Repsol\"</td>\n",
       "      <td>Wrong key</td>\n",
       "      <td>1403913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[7, 10]</td>\n",
       "      <td>3</td>\n",
       "      <td>I got this version instead of the PS3 version,...</td>\n",
       "      <td>09 14, 2011</td>\n",
       "      <td>A1DLMTOTHQ4AST</td>\n",
       "      <td>ampgreen</td>\n",
       "      <td>awesome game, if it did not crash frequently !!</td>\n",
       "      <td>1315958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>I had Dirt 2 on Xbox 360 and it was an okay ga...</td>\n",
       "      <td>06 14, 2011</td>\n",
       "      <td>A361M14PU2GUEG</td>\n",
       "      <td>Angry Ryan \"Ryan A. Forrest\"</td>\n",
       "      <td>DIRT 3</td>\n",
       "      <td>1308009600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  helpful  overall  \\\n",
       "0  0700099867  [8, 12]        1   \n",
       "1  0700099867   [0, 0]        4   \n",
       "2  0700099867   [0, 0]        1   \n",
       "3  0700099867  [7, 10]        3   \n",
       "4  0700099867   [2, 2]        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Installing the game was a struggle (because of...   07 9, 2012   \n",
       "1  If you like rally cars get this game you will ...  06 30, 2013   \n",
       "2  1st shipment received a book instead of the ga...  06 28, 2014   \n",
       "3  I got this version instead of the PS3 version,...  09 14, 2011   \n",
       "4  I had Dirt 2 on Xbox 360 and it was an okay ga...  06 14, 2011   \n",
       "\n",
       "       reviewerID                        reviewerName  \\\n",
       "0  A2HD75EMZR8QLN                                 123   \n",
       "1  A3UR8NLLY1ZHCX  Alejandro Henao \"Electronic Junky\"   \n",
       "2  A1INA0F5CWW3J4          Amazon Shopper \"Mr.Repsol\"   \n",
       "3  A1DLMTOTHQ4AST                            ampgreen   \n",
       "4  A361M14PU2GUEG        Angry Ryan \"Ryan A. Forrest\"   \n",
       "\n",
       "                                           summary  unixReviewTime  \n",
       "0         Pay to unlock content? I don't think so.      1341792000  \n",
       "1                                  Good rally game      1372550400  \n",
       "2                                        Wrong key      1403913600  \n",
       "3  awesome game, if it did not crash frequently !!      1315958400  \n",
       "4                                           DIRT 3      1308009600  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=training_raw['reviewText'].iloc[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=training_raw['overall'].iloc[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                         'even though', 'yet']\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop if i not in negative])\n",
    "    punc_free = \"\".join([ch for ch in stop_free if ch not in exclude])\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(X):\n",
    "    X[index]=clean(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-80412c7087a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#vector = vectorizer.transform(input_text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# summarize encoded vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_mat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    844\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \"\"\"\n\u001b[1;32m--> 846\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = HashingVectorizer()\n",
    "# encode document\n",
    "vector = vectorizer.transform(X)\n",
    "#vector = vectorizer.transform(input_text)\n",
    "# summarize encoded vector\n",
    "X_mat=vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res= sm.fit_sample(X, y)\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(y_res)\n",
    "y_res=encoder.transform(y_res)\n",
    "y_res=np_utils.to_categorical(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without wrapper\n",
    "classifier=Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 20))\n",
    "\n",
    "#Adding dropout\n",
    "classifier.add(Dropout(0.4, seed=0))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "#Adding the third hidden layer\n",
    "classifier.add(Dense(units=10, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "#Adding dropout\n",
    "classifier.add(Dropout(0.4, seed=0))\n",
    "\n",
    "# Adding the output layer (Softmax because we want probabilities within 0 and 1 )\n",
    "classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN (gradient descent with logarithmic loss)\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.1985\n",
      "Epoch 2/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6095 - acc: 0.2017\n",
      "Epoch 3/100\n",
      "104224/104224 [==============================] - 13s 129us/step - loss: 1.6096 - acc: 0.2001\n",
      "Epoch 4/100\n",
      "104224/104224 [==============================] - 14s 130us/step - loss: 1.6096 - acc: 0.2007\n",
      "Epoch 5/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.1993\n",
      "Epoch 6/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.1995\n",
      "Epoch 7/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.1981\n",
      "Epoch 8/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1995\n",
      "Epoch 9/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.1999\n",
      "Epoch 10/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1983\n",
      "Epoch 11/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1982\n",
      "Epoch 12/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.2003\n",
      "Epoch 13/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.2030\n",
      "Epoch 14/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.2003\n",
      "Epoch 15/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.2009\n",
      "Epoch 16/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6097 - acc: 0.1977\n",
      "Epoch 17/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.2002\n",
      "Epoch 18/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1989\n",
      "Epoch 19/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.2021\n",
      "Epoch 20/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.1989\n",
      "Epoch 21/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.2007\n",
      "Epoch 22/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.2002\n",
      "Epoch 23/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.2017\n",
      "Epoch 24/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.2004\n",
      "Epoch 25/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1999\n",
      "Epoch 26/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1980\n",
      "Epoch 27/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1979\n",
      "Epoch 28/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.2003\n",
      "Epoch 29/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1992\n",
      "Epoch 30/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.2006\n",
      "Epoch 31/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1984\n",
      "Epoch 32/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.1998\n",
      "Epoch 33/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.2026\n",
      "Epoch 34/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.1990\n",
      "Epoch 35/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.2005\n",
      "Epoch 36/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.1975\n",
      "Epoch 37/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.2000\n",
      "Epoch 38/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.2006\n",
      "Epoch 39/100\n",
      "104224/104224 [==============================] - 14s 135us/step - loss: 1.6096 - acc: 0.2012\n",
      "Epoch 40/100\n",
      "104224/104224 [==============================] - 14s 135us/step - loss: 1.6096 - acc: 0.1986\n",
      "Epoch 41/100\n",
      "104224/104224 [==============================] - 14s 135us/step - loss: 1.6096 - acc: 0.2023\n",
      "Epoch 42/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1980\n",
      "Epoch 43/100\n",
      "104224/104224 [==============================] - 14s 134us/step - loss: 1.6096 - acc: 0.1984\n",
      "Epoch 44/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1987\n",
      "Epoch 45/100\n",
      "104224/104224 [==============================] - 14s 138us/step - loss: 1.6096 - acc: 0.1985\n",
      "Epoch 46/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.2000\n",
      "Epoch 47/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6097 - acc: 0.1977\n",
      "Epoch 48/100\n",
      "104224/104224 [==============================] - 14s 135us/step - loss: 1.6096 - acc: 0.1981\n",
      "Epoch 49/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.2005\n",
      "Epoch 50/100\n",
      "104224/104224 [==============================] - 14s 135us/step - loss: 1.6096 - acc: 0.2001\n",
      "Epoch 51/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1976\n",
      "Epoch 52/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1997\n",
      "Epoch 53/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1998\n",
      "Epoch 54/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1993\n",
      "Epoch 55/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.2002\n",
      "Epoch 56/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.2001\n",
      "Epoch 57/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.2000\n",
      "Epoch 58/100\n",
      "104224/104224 [==============================] - 14s 137us/step - loss: 1.6096 - acc: 0.2005\n",
      "Epoch 59/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1975\n",
      "Epoch 60/100\n",
      "104224/104224 [==============================] - 14s 135us/step - loss: 1.6096 - acc: 0.1994\n",
      "Epoch 61/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.2003\n",
      "Epoch 62/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1981\n",
      "Epoch 63/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6097 - acc: 0.1980\n",
      "Epoch 64/100\n",
      "104224/104224 [==============================] - 14s 135us/step - loss: 1.6096 - acc: 0.1996\n",
      "Epoch 65/100\n",
      "104224/104224 [==============================] - 14s 137us/step - loss: 1.6096 - acc: 0.1989\n",
      "Epoch 66/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1998\n",
      "Epoch 67/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1998\n",
      "Epoch 68/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1989\n",
      "Epoch 69/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.2008\n",
      "Epoch 70/100\n",
      "104224/104224 [==============================] - 14s 137us/step - loss: 1.6096 - acc: 0.1991\n",
      "Epoch 71/100\n",
      "104224/104224 [==============================] - 14s 137us/step - loss: 1.6096 - acc: 0.1995\n",
      "Epoch 72/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1999\n",
      "Epoch 73/100\n",
      "104224/104224 [==============================] - 15s 142us/step - loss: 1.6096 - acc: 0.2000\n",
      "Epoch 74/100\n",
      "104224/104224 [==============================] - 15s 143us/step - loss: 1.6096 - acc: 0.2009\n",
      "Epoch 75/100\n",
      "104224/104224 [==============================] - 14s 136us/step - loss: 1.6096 - acc: 0.1974\n",
      "Epoch 76/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1996\n",
      "Epoch 77/100\n",
      "104224/104224 [==============================] - 14s 137us/step - loss: 1.6096 - acc: 0.2003\n",
      "Epoch 78/100\n",
      "104224/104224 [==============================] - 15s 148us/step - loss: 1.6096 - acc: 0.2004\n",
      "Epoch 79/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1996\n",
      "Epoch 80/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1995\n",
      "Epoch 81/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1989\n",
      "Epoch 82/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6097 - acc: 0.1967\n",
      "Epoch 83/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1992\n",
      "Epoch 84/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.2001\n",
      "Epoch 85/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1972\n",
      "Epoch 86/100\n",
      "104224/104224 [==============================] - 14s 131us/step - loss: 1.6096 - acc: 0.2006\n",
      "Epoch 87/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1986\n",
      "Epoch 88/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6095 - acc: 0.2007\n",
      "Epoch 89/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.2010\n",
      "Epoch 90/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1996\n",
      "Epoch 91/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.19811s - loss\n",
      "Epoch 92/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1992\n",
      "Epoch 93/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1984\n",
      "Epoch 94/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.19990s - loss: 1.6096 - acc: 0\n",
      "Epoch 95/100\n",
      "104224/104224 [==============================] - 14s 132us/step - loss: 1.6096 - acc: 0.1993\n",
      "Epoch 96/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6097 - acc: 0.1987\n",
      "Epoch 97/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1985\n",
      "Epoch 98/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1977\n",
      "Epoch 99/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1988\n",
      "Epoch 100/100\n",
      "104224/104224 [==============================] - 14s 133us/step - loss: 1.6096 - acc: 0.1996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x40d6d438>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20486134, 0.20371065, 0.19212134, 0.20246992, 0.19683681],\n",
       "       [0.20486134, 0.20371065, 0.19212134, 0.20246992, 0.19683681],\n",
       "       [0.20486134, 0.20371065, 0.19212134, 0.20246992, 0.19683681],\n",
       "       [0.20486134, 0.20371065, 0.19212134, 0.20246992, 0.19683681],\n",
       "       [0.20486134, 0.20371065, 0.19212134, 0.20246992, 0.19683681],\n",
       "       [0.20486134, 0.20371065, 0.19212134, 0.20246992, 0.19683681],\n",
       "       [0.20486134, 0.20371065, 0.19212134, 0.20246992, 0.19683681]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_predictions=classifier.predict(fake_matrix)\n",
    "fake_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={\"CPU\":  4})\n",
    "keras.backend.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Classification Test</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reuters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF=pd.read_csv('r8-train-stemmed.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF=pd.read_csv('r8-test-stemmed.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=trainDF[1]\n",
    "train_y=trainDF[0]\n",
    "valid_x=testDF[1]\n",
    "valid_y=testDF[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Videogames Amazon</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=pd.read_csv('amazon_reviews_utf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF=DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=DF['reviewText']\n",
    "y=DF['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=preprocessing.LabelEncoder()\n",
    "train_y=encoder.fit_transform(train_y)\n",
    "valid_y=encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(X)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(X)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(X)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "\n",
    "data=zipfile.ZipFile('wiki vec\\wiki-news-300d-1M.vec.zip')\n",
    "for i, line in enumerate(data.open('wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF[1])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.9607126541799909\n",
      "NB, WordLevel TF-IDF:  0.9150296939241663\n",
      "NB, N-Gram Vectors:  0.9214253083599817\n",
      "NB, CharLevel Vectors:  0.8465052535404294\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print( \"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print( \"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initial Reuters dataset: \n",
    "\n",
    "NB, Count Vectors:  0.9607126541799909\n",
    "\n",
    "NB, WordLevel TF-IDF:  0.9150296939241663\n",
    "\n",
    "NB, N-Gram Vectors:  0.9214253083599817\n",
    "\n",
    "NB, CharLevel Vectors:  0.8465052535404294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print( \"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print( \"NB, CharLevel Vectors: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
