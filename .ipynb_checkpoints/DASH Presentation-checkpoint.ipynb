{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Emotion Detection From Text using Neural Networks: My Progress to Date</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    "Before landing on neural networks, I tried other approaches like SVM and Naive Bayes classification, but they performed poorly or not at all. I don't know if that's because they just don't work for this approach or that I implemented them wrong. Probably the latter. They are supposed to work well for polar sentiment classification (positive vs. negative sentiment)- does somebody like this product or hate it? \n",
    "\n",
    "I turned to neural networks some time afterwards and soon got promising results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 1: 6-emotion LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Tools used: \n",
    "keras, scikit-learn, nltk, matplotlib, numpy, pandas, gensim</i>\n",
    "\n",
    "1. Get Vector representations of words. I used ones pre-compiled from wikipedia. This won't have all the slang present in tweets, so when out-of-dictionary words come up, the input is ignored. \n",
    "\n",
    "2. Clean and preprocess- Stopwords, taking into account negation\n",
    "3. Write functions to transform string input into vectors\n",
    "4. Read in the data. \n",
    "5. Neural Network\n",
    "Create a multi-layer LSTM with softmax output layer to output some function output similar to probabilities for each emotion. \n",
    "Initial model had 48% test accuracy. \n",
    "With revisions and more timesteps, I got it up to 87% test accuracy. \n",
    "\n",
    "<h3>Data</h3>\n",
    "I will cut straight to the point: The biggest challenge for my project is scarcity of training data. I have only 7-10k rows at max to train on (row being a sentence or collection of sentences) meaning that there's not enough language variation present for the model to pick up on nuances of speech like negation and sarcasm. Of course, even with gigantic training sets, this very human-like part of speech comprehension is just outside the grasp of cutting edge NLP technology. I initially thought I could make some limited progress here but I've been stymied. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 2: Data Augmentation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, in order to get my neural network to recognize negation (I am not happy is different than I am happy) I created fake data with \"not\" + emotional words and assigned the correct emotion to each pair. For example, one synthetic row would be 'not joyful not joyful not joyful' (repeated three times because of the format of my LSTM) and the emotion for that synthetic row would be sadness. \n",
    "I created around 3000 examples of this data, fed it to the LSTM, and presto! It could recognize negation. \n",
    "\n",
    "I didn't want to do this approach much more because that would mean I would have to create data to represent variation in human language, which I felt would be too hard-coded and old fashioned for the 21st century. So I tried to find techniques that would help my LSTM automatically recognize nuances in speech even with small training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 3: Spacy NLP</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used part-of-speech tagging, dependency parsing in sentence structure, along with the existing format of six timesteps. This was a pain to program because it would seem the parts aren't made for each other. When I finally got it to run, the end result was at least 10 percentage points less accurate than my previous model despite it having more structured information for each timestep. And no, it did not pick up on negation. So I moved onto Attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Phase 4: Attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what I was talking about a month ago that I thought might help me break through. I implemented it and as far as I can tell it's comparable, but less accurate than my previous best. \n",
    "\n",
    "Here's attention in a nutshell: \n",
    "\n",
    "First, the LSTM layer produces a matrix H of output vectors of length T (T being the sentence length, because attention is implemented per-sentence.)\n",
    "\n",
    "H is within (dimension of word vectors * sentence length) dimensional space. \n",
    "\n",
    "The representation of the sentence is formed like such: \n",
    "\n",
    "M= tanh(H), tanh being an activation function which is steep between -1 and 1, but converges to -1 and 1 past x=2, -2. So strongly negative inputs will be mapped strongly negative and zero inputs will be mapped near zero. (whatever that means in application, I'm not sure.)\n",
    "\n",
    "alpha= softmax(transpose of weight, which gets updated with every pass by the attention mechanism * M from the previous step)\n",
    "softmax is another activation function\n",
    "\n",
    "r=H*transpose of alpha\n",
    "\n",
    "finally, \"We obtain the final sentence-pair represnetation used for classification  from \n",
    "\n",
    "h* = tanh(r)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
